{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore');\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/Hilak/Desktop/INTERESTS/Machine Learning A-Z Template Folder/Part 3 - Classification/Section 14 - Logistic Regression\");\n",
    "training_set = pd.read_csv(\"Social_Network_Ads.csv\");\n",
    "X = training_set.iloc[:, 1:4].values\n",
    "y = training_set.iloc[:, 4].values\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "le_x = LabelEncoder()\n",
    "X[:,0] = le_x.fit_transform(X[:,0])\n",
    "ohe = OneHotEncoder(categorical_features = [0])\n",
    "X = ohe.fit_transform(X).toarray()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "X[:,2:4] = ss.fit_transform(X[:, 2:4])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate Dataset for test purposes. Not used in the example shown\n",
    "# os.chdir(\"C:\\\\Users\\\\Hilak\\\\Desktop\\\\INTERESTS\\\\Machine Learning A-Z Template Folder\\\\Part 8 - Deep Learning\\\\Section 39 - Artificial Neural Networks (ANN)\");\n",
    "# dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "# X = dataset.iloc[:, 3:13].values\n",
    "# y = dataset.iloc[:, 13].values\n",
    "\n",
    "# # Encoding categorical data\n",
    "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# labelencoder_X_1 = LabelEncoder()\n",
    "# X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "# labelencoder_X_2 = LabelEncoder()\n",
    "# X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "# onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "# X = onehotencoder.fit_transform(X).toarray()\n",
    "# X = X[:, 1:]\n",
    "\n",
    "# # Splitting the dataset into the Training set and Test set\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "# X_test, X_CV, y_test, y_CV = train_test_split(X, y, test_size = 0.5)\n",
    "\n",
    "# # Feature Scaling\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)\n",
    "\n",
    "# X_train = X_train.T\n",
    "# X_test = X_test.T\n",
    "# X_CV = X_CV.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z) : \n",
    "    return 1./(1 + np.exp(-z))\n",
    "def sigmoid_prime(z) :\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "def ReLU(z) : \n",
    "    return (z*(z > 0))\n",
    "def ReLU_prime(z) :\n",
    "    return 1*(z>=0)\n",
    "def lReLU(z) : \n",
    "    return np.maximum(z/100,z)\n",
    "def lReLU_prime(z) :\n",
    "    z = 1*(z>=0)\n",
    "    z[z==0] = 1/100\n",
    "    return z\n",
    "def tanh(z) : \n",
    "    return np.tanh(z)\n",
    "def tanh_prime(z) : \n",
    "    return (1-tanh(z)**2)\n",
    "PHI = {'sigmoid':sigmoid, 'relu':ReLU, 'lrelu':lReLU, 'tanh':tanh}\n",
    "PHI_PRIME = {'sigmoid':sigmoid_prime, 'relu':ReLU_prime, 'lrelu':lReLU_prime, 'tanh':tanh_prime}\n",
    "class NeuralNet : \n",
    "    def __init__(self, layers, X, y, ac_funcs, init_method='gaussian', loss_func='b_ce', W=np.array([]), B=np.array([])) : \n",
    "        self.layers = layers\n",
    "        self.W = None\n",
    "        self.B = None\n",
    "        self.m = X.shape[1]\n",
    "        self.n = [X.shape[0], *layers]\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.cost = []\n",
    "        self.acc = 0\n",
    "        self.ac_funcs = ac_funcs\n",
    "        self.loss = loss_func\n",
    "        if len(W) and len(B) :\n",
    "            self.W = W\n",
    "            self.B = B\n",
    "        else : \n",
    "            if init_method=='gaussian': \n",
    "                self.W = [np.random.randn(self.n[nl], self.n[nl-1]) for nl in range(1,len(self.n))]\n",
    "                self.B = [np.zeros((nl,1), 'float32') for nl in self.layers]\n",
    "            elif init_method == 'random':\n",
    "                self.W = [np.random.rand(self.n[nl], self.n[nl-1]) for nl in range(1,len(self.n))]\n",
    "                self.B = [np.random.rand(nl,1) for nl in self.layers]\n",
    "            elif init_method == 'zeros':\n",
    "                self.W = [np.zeros((self.n[nl], self.n[nl-1]), 'float32') for nl in range(1,len(self.n))]\n",
    "                self.B = [np.zeros((nl,1), 'float32') for nl in self.layers]\n",
    "    \n",
    "    def startTraining(self, epochs, alpha, _lambda, keep_prob=0.5, interval=100):\n",
    "        start = time.time()\n",
    "        for i in range(epochs+1) : \n",
    "            z,a = self._feedForward(keep_prob)\n",
    "            delta = self._cost_derivative(a[-1])\n",
    "            for l in range(1,len(z)) : \n",
    "                delta_w = np.dot(delta, a[-l-1].T) + (_lambda)*self.W[-l]\n",
    "                delta_b = np.dot(delta, np.ones((self.m,1), 'int32'))\n",
    "                self.W[-l] = self.W[-l] - (alpha/self.m)*delta_w\n",
    "                self.B[-l] = self.B[-l] - (alpha/self.m)*delta_b\n",
    "                delta = np.dot(self.W[-l].T, delta)*PHI_PRIME[self.ac_funcs[-l-1]](z[-l-1])\n",
    "            if not i%interval :\n",
    "                aa = self.predict(self.X)\n",
    "                aa = aa > 0.5\n",
    "                self.acc = sum(sum(aa == self.y)) / self.m\n",
    "                cost_val = self._cost_func(a[-1], _lambda)\n",
    "                self.cost.append(cost_val)\n",
    "            sys.stdout.write(f'\\rEpoch[{i}] : Cost = {cost_val:.2f} ; Acc = {(self.acc*100):.2f}% ; Time Taken = {(time.time()-start):.2f}s')\n",
    "        print('\\n')\n",
    "        return None\n",
    "    \n",
    "    def predict(self, X_test) : \n",
    "        a = PHI[self.ac_funcs[0]](np.dot(self.W[0], X_test) + self.B[0])\n",
    "        for l in range(1,len(self.layers)):\n",
    "            a = PHI[self.ac_funcs[l]](np.dot(self.W[l], a) + self.B[l])\n",
    "        return a\n",
    "            \n",
    "    \n",
    "    def _feedForward(self, keep_prob):\n",
    "        z = [];a = []\n",
    "        z.append(np.dot(self.W[0], self.X) + self.B[0])\n",
    "        a.append(PHI[self.ac_funcs[0]](z[0]))\n",
    "        for l in range(1,len(self.layers)):\n",
    "            z.append(np.dot(self.W[l], a[l-1]) + self.B[l])\n",
    "            # a.append(PHI[self.ac_funcs[l]](z[l]))\n",
    "            _a = PHI[self.ac_funcs[l]](z[l])\n",
    "            a.append( ((np.random.rand(*_a.shape) < keep_prob)*_a)/keep_prob )\n",
    "        return z,a\n",
    "    \n",
    "    def _cost_func(self, a, _lambda):\n",
    "        return ( (-1/self.m)*np.sum(np.nan_to_num(self.y*np.log(a) + (1-self.y)*np.log(1-a))) + (_lambda/(2*self.m))*np.sum([np.sum(i**2) for i in self.W]) )\n",
    "\n",
    "    def _cost_derivative(self, a) : \n",
    "        return (a-self.y)\n",
    "   \n",
    "    @property\n",
    "    def summary(self) :\n",
    "        return self.cost, self.acc, self.W,self.B\n",
    "    def __repr__(self) : \n",
    "        return f'<UNDER CONST>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[5000] : Cost = inf ; Acc = 90.71% ; Time Taken = 23.11s\n",
      "\n",
      "Test set Accuracy ( r-t-s ) : 94.16666666666667%\n"
     ]
    }
   ],
   "source": [
    "neural_net_sigmoid = NeuralNet([64,32,1], X_train, y_train, ac_funcs = ['relu','tanh','sigmoid'])\n",
    "neural_net_sigmoid.startTraining(5000, 0.5, 0.5, 0.45, 100)\n",
    "preds = neural_net_sigmoid.predict(X_test)\n",
    "preds = preds > 0.5\n",
    "acc = (sum(sum(preds == y_test)) / y_test.size)*100\n",
    "print(f'Test set Accuracy ( r-t-s ) : {acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sigmoid_summary = neural_net_sigmoid.summary\n",
    "plt.plot(range(len(sigmoid_summary[0])), sigmoid_summary[0], label='Sigmoid Cost')\n",
    "plt.title('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "280/280 [==============================] - 4s 15ms/step - loss: 0.6115 - acc: 0.7464\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - 2s 6ms/step - loss: 0.3772 - acc: 0.8214\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - 2s 6ms/step - loss: 0.3372 - acc: 0.8536\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - 1s 5ms/step - loss: 0.3187 - acc: 0.8750\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - 1s 4ms/step - loss: 0.3060 - acc: 0.8786A: 1s - loss: \n",
      "Epoch 6/10\n",
      "280/280 [==============================] - 1s 3ms/step - loss: 0.2951 - acc: 0.8643\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - 1s 3ms/step - loss: 0.2892 - acc: 0.8714\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.2799 - acc: 0.8857\n",
      "Epoch 9/10\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.2737 - acc: 0.8857\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.2724 - acc: 0.8893\n",
      "Test set Accuracy : 90.0%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "X_train, X_test = X_train.T, X_test.T\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(input_dim=4, units = 32, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
    "classifier.add(Dense(units = 16, kernel_initializer = \"uniform\", activation=\"relu\"))\n",
    "# classifier.add(Dense(units = 16, kernel_initializer = \"uniform\", activation=\"tanh\"))\n",
    "classifier.add(Dense(units = 1, kernel_initializer = \"uniform\", activation = \"sigmoid\"))\n",
    "classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "classifier.fit(X_train, y_train, batch_size = 1, epochs = 10)\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = 1*(y_pred > 0.5)\n",
    "test_acc = sum(sum(y_pred.T == y_test)) / y_test.size\n",
    "print(f\"Test set Accuracy : {test_acc*100}%\")\n",
    "X_train, X_test = X_train.T, X_test.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
