{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import sys\n",
    "from concurrent.futures import ProcessPoolExecutor as ppe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alternate Dataset for test purposes. Not used in the example shown\n",
    "# # Importing our dataset\n",
    "# os.chdir(\"C:/Users/Hilak/Desktop/INTERESTS/Machine Learning A-Z Template Folder/Part 3 - Classification/Section 14 - Logistic Regression\");\n",
    "# training_set = pd.read_csv(\"Social_Network_Ads.csv\");\n",
    "\n",
    "# # Splitting our dataset into matrix of features and output values.\n",
    "# X = training_set.iloc[:, 1:4].values\n",
    "# y = training_set.iloc[:, 4].values\n",
    "\n",
    "# # Encoding our object features.\n",
    "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# le_x = LabelEncoder()\n",
    "# X[:,0] = le_x.fit_transform(X[:,0])\n",
    "# ohe = OneHotEncoder(categorical_features = [0])\n",
    "# X = ohe.fit_transform(X).toarray()\n",
    "\n",
    "# # Performing Feature scaling\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# ss = StandardScaler()\n",
    "# X[:,2:4] = ss.fit_transform(X[:, 2:4])\n",
    "\n",
    "# # Splitting the dataset into train and test set.\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n",
    "# X_train = X_train.T\n",
    "# X_test = X_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big><b>This dataset contains the click of a user on an advertisement of a new brand car</b></big>.<br> \n",
    "The dataset has the following features:\n",
    "1. User ID.\n",
    "2. Gender.\n",
    "3. Age.\n",
    "4. Estimated Saalry.\n",
    "5. Purchsed - Boolean indicating if the user bought the car.\n",
    "\n",
    "<br>\n",
    "<b>Size of the dataset : 400</b>\n",
    "<br>\n",
    "Except User ID, we will be keeping all the features to train our neural network. Although, during EDA, it has been found that \"estimated salary\" causes some overfitting. So, it would be a good dataset to make our model more robust so it deals with similar situation by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\Hilak\\\\Desktop\\\\INTERESTS\\\\Machine Learning A-Z Template Folder\\\\Part 8 - Deep Learning\\\\Section 39 - Artificial Neural Networks (ANN)\");\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, [13]].values\n",
    "\n",
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Problem statement: This dataset contains the data of customers of a bank. The bank has hired you as a data scientist to predict if the customer will stay with the bank or leave.<br><br>\n",
    "<strong>Total no. of examples in the dataset     : 10000</strong><br>\n",
    "<strong>No. of examples in the training set      : 8000</strong><br>\n",
    "<strong>No. of examples in the test set          : 1000</strong><br>\n",
    "<strong>No. of examples in the CV set            : 1000</strong><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z) :\n",
    "    \"\"\" Reutrns the element wise sigmoid function. \"\"\"\n",
    "    return 1./(1 + np.exp(-z))\n",
    "def sigmoid_prime(z) :\n",
    "    \"\"\" Returns the derivative of the sigmoid function. \"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "def ReLU(z) : \n",
    "    \"\"\" Reutrns the element wise ReLU function. \"\"\"\n",
    "    return (z*(z > 0))\n",
    "def ReLU_prime(z) :\n",
    "    \"\"\" Returns the derivative of the ReLU function. \"\"\"\n",
    "    return 1*(z>=0)\n",
    "def lReLU(z) : \n",
    "    \"\"\" Reutrns the element wise leaky ReLU function. \"\"\"\n",
    "    return np.maximum(z/100,z)\n",
    "def lReLU_prime(z) :\n",
    "    \"\"\" Returns the derivative of the leaky ReLU function. \"\"\"\n",
    "    z = 1*(z>=0)\n",
    "    z[z==0] = 1/100\n",
    "    return z\n",
    "def tanh(z) :\n",
    "    \"\"\" Reutrns the element wise hyperbolic tangent function. \"\"\"\n",
    "    return np.tanh(z)\n",
    "def tanh_prime(z) : \n",
    "    \"\"\" Returns the derivative of the tanh function. \"\"\"\n",
    "    return (1-tanh(z)**2)\n",
    "def softmax(z) :\n",
    "    t = np.exp(z)\n",
    "    return (t/np.sum(t,axis=0))\n",
    "def softmax_prime(z):\n",
    "    return softmax(z)*(1-softmax(z))\n",
    "    \n",
    "\n",
    "# A dictionary of our activation functions\n",
    "PHI = {'sigmoid':sigmoid, 'relu':ReLU, 'lrelu':lReLU, 'tanh':tanh, 'softmax':softmax}\n",
    "\n",
    "# A dictionary containing the derivatives of our activation functions\n",
    "PHI_PRIME = {'sigmoid':sigmoid_prime, 'relu':ReLU_prime, 'lrelu':lReLU_prime, 'tanh':tanh_prime, 'softmax':softmax_prime}\n",
    "\n",
    "\n",
    "class NeuralNet : \n",
    "    \"\"\"\n",
    "    This is a class for implementing Artificial Neural Networks. L2 and Droupout are the \n",
    "    default regularization methods implemented in this class. It takes the following parameters:\n",
    "    \n",
    "    1. layers      : A python list containing the different number of neurons in each layer.\n",
    "                     (containing the output layer)\n",
    "                     Eg - [64,32,16,16,1]\n",
    "                \n",
    "    2. X           : Matrix of features with rows as features and columns as different examples.\n",
    "    \n",
    "    3. y           : Numpy array containing the ouputs of coresponding examples.\n",
    "    \n",
    "    4. ac_funcs    : A python list containing activation function of each layer.\n",
    "                     Eg - ['relu','relu','lrelu','tanh','sigmoid']\n",
    "    \n",
    "    5. init_method : Meathod to initialize weights of the network. Can be 'gaussian','random','zeros'.\n",
    "    \n",
    "    6. loss_func   : b_ce = Binary Cross Entropy cost function for binary classification. Similarly, c_ce for categorical classification.\n",
    "    \n",
    "    7. W           : Weights of a pretrained neural network with same architecture.\n",
    "    \n",
    "    8. B           : Biases of a pretrained neural network with same architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, X, y, ac_funcs, init_method='gaussian', loss_func='b_ce', W=np.array([]), B=np.array([])) :\n",
    "        \"\"\" Initialize the network. \"\"\"\n",
    "        # Store the layers of the network\n",
    "        self.layers = layers\n",
    "        # ----\n",
    "        self.W = None\n",
    "        self.B = None\n",
    "        # Store the number of examples in the dataset as m\n",
    "        self.m = X.shape[1]\n",
    "        # Store the full layer list as n\n",
    "        self.n = [X.shape[0], *layers]\n",
    "        # Save the dataset\n",
    "        self.X = X\n",
    "        # Save coresponding output\n",
    "        self.y = y\n",
    "        self.X_mini = None\n",
    "        self.y_mini = None\n",
    "        self.m_mini = None\n",
    "        # List to store the cost of the model calculated during training\n",
    "        self.cost = []\n",
    "        # Stores the accuracy obtained on the test set.\n",
    "        self.acc = 0\n",
    "        # Activation function of each layer\n",
    "        self.ac_funcs = ac_funcs\n",
    "        self.loss = loss_func\n",
    "        # Initialize the weights by provided method.\n",
    "        if len(W) and len(B) :\n",
    "            self.W = W\n",
    "            self.B = B\n",
    "        else : \n",
    "            if init_method=='gaussian': \n",
    "                self.W = [np.random.randn(self.n[nl], self.n[nl-1])*np.sqrt(2/self.n[nl-1]) for nl in range(1,len(self.n))]\n",
    "                self.B = [np.zeros((nl,1), 'float32') for nl in self.layers]\n",
    "            elif init_method == 'random':\n",
    "                self.W = [np.random.rand(self.n[nl], self.n[nl-1]) for nl in range(1,len(self.n))]\n",
    "                self.B = [np.random.rand(nl,1) for nl in self.layers]\n",
    "            elif init_method == 'zeros':\n",
    "                self.W = [np.zeros((self.n[nl], self.n[nl-1]), 'float32') for nl in range(1,len(self.n))]\n",
    "                self.B = [np.zeros((nl,1), 'float32') for nl in self.layers]\n",
    "        self.vdw = [np.zeros(i.shape) for i in self.W]\n",
    "        self.vdb = [np.zeros(i.shape) for i in self.B]\n",
    "        self.sdw = [np.zeros(i.shape) for i in self.W]\n",
    "        self.sdb = [np.zeros(i.shape) for i in self.B]\n",
    "    \n",
    "    def startTraining(self, batch_size, epochs, alpha, decay_rate, _lambda, keep_prob, beta1=0.9, beta2=0.999, interval=10, print_metrics = True, evaluate=False, X_test=None, y_test=None):\n",
    "        \"\"\"\n",
    "        Start training the neural network. It takes the followng parameters : \n",
    "        \n",
    "        1. batch_size : Size of your mini batch. Must be greater than 1.\n",
    "        \n",
    "        2. epochs     : Number of epochs for which you want to train the network.\n",
    "        \n",
    "        3. alpha      : The learning rate of your network.\n",
    "        \n",
    "        4. decay_rate : The rate at which you want to decrease your learning rate.\n",
    "\n",
    "        5. _lambda    : L2 regularization parameter or the penalization parameter.\n",
    "\n",
    "        6. keep_prob  : Python List. Dropout regularization parameter. The percentage of neurons to keep activated.\n",
    "                        Eg - 0.8 means 20% of the neurons have been deactivated.\n",
    "        \n",
    "        7. beta1      : Momentum. default=0.9\n",
    "        \n",
    "        8. beta2      : RMSprop Parameter. default=0.999\n",
    "        \n",
    "        9. interval   : The interval between updates of cost and accuracy.\n",
    "        \"\"\"\n",
    "        print('Starting Training:')\n",
    "        dataset_size = self.X.shape[1]\n",
    "        k=1\n",
    "        cost_val = 0\n",
    "        for j in range(1, epochs+1):\n",
    "            start = time.time()\n",
    "            alpha = alpha/( 1 + decay_rate*(j-1) )\n",
    "            for i in range(0, dataset_size-batch_size+1, batch_size):\n",
    "                self.X_mini = self.X[:, i:i+batch_size]\n",
    "                self.y_mini = self.y[:, i:i+batch_size]\n",
    "                self.m_mini = self.y_mini.shape[1]\n",
    "                self._miniBatchTraining(alpha, beta1, beta2, _lambda, keep_prob,k)\n",
    "                aa = self.predict(self.X)\n",
    "                if not k%interval:\n",
    "                    if self.loss == 'b_ce':\n",
    "                        aa_ = aa > 0.5\n",
    "                        self.acc = np.sum(aa_ == self.y) / self.m\n",
    "                        cost_val = self._cost_func(aa, _lambda)\n",
    "                        self.cost.append(cost_val)\n",
    "                    elif self.loss == 'c_ce':\n",
    "                        aa_ = np.argmax(aa, axis = 0)\n",
    "                        yy = np.argmax(self.y, axis = 0)\n",
    "                        self.acc = np.sum(aa_==yy)/(self.m)\n",
    "                        cost_val = self._cost_func(aa, _lambda)\n",
    "                        self.cost.append(cost_val)\n",
    "                if print_metrics:\n",
    "                    sys.stdout.write(f'\\rEpoch[{j}] {i+batch_size}/{self.m} : Cost = {cost_val:.4f} ; Acc = {(self.acc*100):.2f}% ; Time Taken = {(time.time()-start):.0f}s')\n",
    "                    print(\"\\n\")\n",
    "                k+=1\n",
    "        if evaluate:\n",
    "            print(f\"For batch_size = {batch_size}, epochs = {epochs}, alpha = {alpha}, decay_rate = {decay_rate}, _lambda = {_lambda}, keep_prob = {keep_prob}\")\n",
    "            aa = self.predict(X_test)\n",
    "            if self.loss == 'b_ce':\n",
    "                aa_ = aa > 0.5\n",
    "                acc = np.sum(aa_ == y_test) / X_test.shape[1]\n",
    "                print(f\"Accuracy on training set: {self.acc}\")\n",
    "                print(f\"Accuracy on test set: {acc}\\n\")\n",
    "            elif self.loss == 'c_ce':\n",
    "                aa_ = np.argmax(aa, axis = 0)\n",
    "                yy = np.argmax(y_test, axis = 0)\n",
    "                acc = np.sum(aa_==yy)/(X_test.shape[1])\n",
    "                print(f\"Accuracy on training set: {self.acc}\")\n",
    "                print(f\"Accuracy on test set: {acc}\\n\")\n",
    "        \n",
    "    \n",
    "    def _miniBatchTraining(self, alpha, beta1, beta2, _lambda, keep_prob,i):\n",
    "        epsilon = 1e-8\n",
    "        z,a = self._feedForward(keep_prob)\n",
    "        delta = self._cost_derivative(a[-1])\n",
    "        for l in range(1,len(z)) : \n",
    "            delta_w = (1/self.m_mini)*(np.dot(delta, a[-l-1].T) + (_lambda)*self.W[-l])\n",
    "            delta_b = (1/self.m_mini)*(np.sum(delta, axis=1, keepdims=True))\n",
    "            self.vdw[-l] = (beta1*self.vdw[-l] + (1-beta1)*delta_w)\n",
    "            vdw_corrected = self.vdw[-l]/(1 - beta1**i)\n",
    "            self.vdb[-l] = (beta1*self.vdb[-l] + (1-beta1)*delta_b)\n",
    "            vdb_corrected = self.vdb[-l]/(1 - beta1**i)\n",
    "            self.sdw[-l] = (beta2*self.sdw[-l] + (1-beta2)*(delta_w**2))\n",
    "            sdw_corrected = self.sdw[-l]/(1 - beta2**i)\n",
    "            self.sdb[-l] = (beta2*self.sdb[-l] + (1-beta2)*(delta_b**2))\n",
    "            sdb_corrected = self.sdb[-l]/(1 - beta2**i)\n",
    "            delta = np.dot(self.W[-l].T, delta)*PHI_PRIME[self.ac_funcs[-l-1]](z[-l-1])\n",
    "            self.W[-l] = self.W[-l] - (alpha)*(vdw_corrected/(np.sqrt(sdw_corrected)+epsilon))\n",
    "            self.B[-l] = self.B[-l] - (alpha)*(vdb_corrected/(np.sqrt(sdb_corrected)+epsilon))\n",
    "        delta_w = (1/self.m_mini)*(np.dot(delta, self.X_mini.T ) + (_lambda)*self.W[0])\n",
    "        delta_b = (1/self.m_mini)*(np.sum(delta, axis=1, keepdims=True))\n",
    "        self.vdw[0] = (beta1*self.vdw[0] + (1-beta1)*delta_w)\n",
    "        vdw_corrected = self.vdw[0]/(1 - beta1**i)\n",
    "        self.vdb[0] = (beta1*self.vdb[0] + (1-beta1)*delta_b)\n",
    "        vdb_corrected = self.vdb[0]/(1 - beta1**i)\n",
    "        self.sdw[0] = (beta2*self.sdw[0] + (1-beta2)*(delta_w**2))\n",
    "        sdw_corrected = self.sdw[0]/(1 - beta2**i)\n",
    "        self.sdb[0] = (beta2*self.sdb[0] + (1-beta2)*(delta_b**2))\n",
    "        sdb_corrected = self.sdb[0]/(1 - beta2**i)\n",
    "        self.W[0] = self.W[0] - (alpha)*(vdw_corrected/(np.sqrt(sdw_corrected)+epsilon))\n",
    "        self.B[0] = self.B[0] - (alpha)*(vdb_corrected/(np.sqrt(sdb_corrected)+epsilon))\n",
    "        return None\n",
    "    \n",
    "    def predict(self, X_test) :\n",
    "        \"\"\" Predict the labels for a new dataset. Returns probability. \"\"\"\n",
    "        a = PHI[self.ac_funcs[0]](np.dot(self.W[0], X_test) + self.B[0])\n",
    "        for l in range(1,len(self.layers)):\n",
    "            a = PHI[self.ac_funcs[l]](np.dot(self.W[l], a) + self.B[l])\n",
    "        return a\n",
    "            \n",
    "    \n",
    "    def _feedForward(self, keep_prob):\n",
    "        \"\"\" Forward pass \"\"\"\n",
    "        z = [];a = []\n",
    "        z.append(np.dot(self.W[0], self.X_mini) + self.B[0])\n",
    "        a.append(PHI[self.ac_funcs[0]](z[-1]))\n",
    "        for l in range(1,len(self.layers)-1):\n",
    "            z.append(np.dot(self.W[l], a[-1]) + self.B[l])\n",
    "            # a.append(PHI[self.ac_funcs[l]](z[l]))\n",
    "            _a = PHI[self.ac_funcs[l]](z[l])\n",
    "            a.append( ((np.random.rand(*_a.shape) < keep_prob[l-1])*_a)/keep_prob[l-1] )\n",
    "        z.append(np.dot(self.W[-1], a[-1]) + self.B[-1])\n",
    "        a.append(PHI[self.ac_funcs[-1]](z[-1]))\n",
    "        return z,a\n",
    "    \n",
    "    def _cost_func(self, a, _lambda):\n",
    "        \"\"\" Cross Entropy Cost Function \"\"\"\n",
    "        if self.ac_funcs[-1] == 'sigmoid':\n",
    "            return ( (-1/self.m)*np.sum(np.nan_to_num(self.y*np.log(a+10e-8) + (1-self.y)*np.log(1-a+10e-8))) + (_lambda/(2*self.m))*np.sum([np.sum(i**2) for i in self.W]) )\n",
    "        return ( (-1/self.m)*np.sum(np.nan_to_num(self.y*np.log(a))) + (_lambda/(2*self.m))*np.sum([np.sum(i**2) for i in self.W]) )\n",
    "        \n",
    "    def _cost_derivative(self, a) : \n",
    "        \"\"\" The derivative of cost w.r.t z \"\"\"\n",
    "        return (a-self.y_mini)\n",
    "   \n",
    "    @property\n",
    "    def summary(self) :\n",
    "        return self.cost, self.acc, self.W,self.B\n",
    "    def __repr__(self) : \n",
    "        return f'<Neural Network at {id(self)}>'\n",
    "\n",
    "\n",
    "class HyperParameterTuning:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def GridSearch(self, layers, X, y, ac_funcs, params, X_test, y_test):\n",
    "        if __name__ == '__main__':\n",
    "            models=[]\n",
    "            with ppe(max_workers = len(params)) as pool:\n",
    "                for param in params:\n",
    "                    models.append(NeuralNet(layers, X, y, ac_funcs))\n",
    "                    pool.submit(models[-1].startTraining, batch_size=param['batch_size'], epochs=param['epochs'], alpha=param['alpha'], decay_rate=param['decay_rate'], _lambda=param['_lambda'], keep_prob=param['keep_prob'], print_metrics=False, evaluate=True, X_test=X_test, y_test=y_test)\n",
    "\n",
    "    def RandomizedGridSearch(self, layers, X, y, ac_funcs, params_range, nb_models, X_test, y_test):\n",
    "        if __name__ == '__main__':\n",
    "            models = []\n",
    "            params = []\n",
    "            for i in range(nb_models):\n",
    "                params.append({\n",
    "                    'batch_size' : int(np.round(np.random.rand()*(params_range['batch_size'][1]-params_range['batch_size'][0]) + params_range['batch_size'][0])),\n",
    "                    'epochs' : int(np.round(np.random.rand()*(params_range['epochs'][1]-params_range['epochs'][0]) + params_range['epochs'][0])),\n",
    "                    'alpha' : 10**(np.random.rand()*(np.log10(params_range['alpha'][1])-np.log10(params_range['alpha'][0])) + np.log10(params_range['alpha'][0])),\n",
    "                    'decay_rate' : 10**(np.random.rand()*(np.log10(params_range['decay_rate'][1])-np.log10(params_range['decay_rate'][0])) + np.log10(params_range['decay_rate'][0])),\n",
    "                    '_lambda' : 10**(np.random.rand()*(np.log10(params_range['_lambda'][1])-np.log10(params_range['_lambda'][0])) + np.log10(params_range['_lambda'][0])),\n",
    "                    'keep_prob' : [(np.random.rand()*(params_range['keep_prob'][1]-params_range['keep_prob'][0]) + params_range['keep_prob'][0]) for j in range(len(layers)-1)]\n",
    "                    })\n",
    "            with ppe(max_workers = len(params)) as pool:\n",
    "                for param in params:\n",
    "                    models.append(NeuralNet(layers, X, y, ac_funcs))\n",
    "                    pool.submit(models[-1].startTraining, batch_size=param['batch_size'], epochs=param['epochs'], alpha=param['alpha'], decay_rate=param['decay_rate'], _lambda=param['_lambda'], keep_prob=param['keep_prob'], print_metrics=False, evaluate=True, X_test=X_test, y_test=y_test)\n",
    "        \n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'<HPT at {id(self)}>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training:\n",
      "For batch_size = 500, epochs = 100, alpha = 0.05, decay_rate = 0, _lambda = 0.5, keep_prob = [0.86, 0.9]\n",
      "Accuracy on training set: 0.863\n",
      "Accuracy on test set: 0.8655\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initializing our neural network\n",
    "neural_net_sigmoid = NeuralNet([16,16,1], X_train, y_train, ac_funcs = ['relu','relu','sigmoid'])\n",
    "# Staring the training of our network.\n",
    "neural_net_sigmoid.startTraining(batch_size=500, epochs=100, alpha=0.05, decay_rate=0, beta1=0.9, beta2=0.999, _lambda=0.5, keep_prob=[0.86,0.9], interval=10, print_metrics=False, evaluate=True, X_test=X_test, y_test=y_test)\n",
    "# Predicting on new dataset using our trained network.\n",
    "# preds = neural_net_sigmoid.predict(X_test)\n",
    "# preds = preds > 0.5\n",
    "# acc = (sum(sum(preds == y_test)) / y_test.size)*100\n",
    "# # Accuracy (metric of evaluation) obtained by the network.\n",
    "# print(f'Test set Accuracy : {acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning for batch_size, epochs, alpha, decay_rate, _lambda, keep_prob\n",
    "params = [\n",
    "            {'batch_size':100,'epochs':20,'alpha':0.1,'decay_rate':0,'_lambda':0.5,'keep_prob':[0.86,0.9,1]},\n",
    "            {'batch_size':200,'epochs':40,'alpha':0.1,'decay_rate':0.001,'_lambda':0.7,'keep_prob':[0.8,0.9,1]},\n",
    "            {'batch_size':300,'epochs':60,'alpha':0.07,'decay_rate':0.001,'_lambda':0.3,'keep_prob':[0.9,0.9,1]},\n",
    "            {'batch_size':400,'epochs':80,'alpha':0.05,'decay_rate':0,'_lambda':1,'keep_prob':[0.75,0.95,1]},\n",
    "            {'batch_size':500,'epochs':100,'alpha':0.04,'decay_rate':0,'_lambda':0.8,'keep_prob':[0.5,0.95,1]},\n",
    "            {'batch_size':600,'epochs':120,'alpha':0.03,'decay_rate':0,'_lambda':0.9,'keep_prob':[0.85,0.85,1]},\n",
    "            {'batch_size':700,'epochs':150,'alpha':0.03,'decay_rate':0,'_lambda':1,'keep_prob':[0.8,0.9,1]},\n",
    "            {'batch_size':800,'epochs':200,'alpha':0.02,'decay_rate':0,'_lambda':1.5,'keep_prob':[0.9,0.9,1]}\n",
    "         ]\n",
    "hpt = HyperParameterTuning()\n",
    "hpt.GridSearch([16,16,1], X_train, y_train, ['relu','relu','sigmoid'], params, X_test, y_test)\n",
    "\n",
    "# Doesn't work in Anaconda. Try in your local computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized Grid Search Testing. Doesn't work in Anaconda.\n",
    "params_range = {'batch_size':[100, 1000],'epochs':[20,200],'alpha':[0.001, 0.02],'decay_rate':[0.000001, 0.0001],'_lambda':[0.1, 1.5],'keep_prob':[0.75, 1]}\n",
    "\n",
    "hpt = HyperParameterTuning()\n",
    "hpt.RandomizedGridSearch([16,16,1], X_train, y_train, ['relu','relu','sigmoid'], params_range, 20, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting our cost vs epochs relationship\n",
    "sigmoid_summary = neural_net_sigmoid.summary\n",
    "plt.plot(range(len(sigmoid_summary[0])), sigmoid_summary[0], label='Sigmoid Cost')\n",
    "plt.title('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 0.6905 - acc: 0.7515\n",
      "Epoch 2/200\n",
      "8000/8000 [==============================] - 0s 18us/step - loss: 0.6830 - acc: 0.7925\n",
      "Epoch 3/200\n",
      "8000/8000 [==============================] - 0s 18us/step - loss: 0.6702 - acc: 0.7925\n",
      "Epoch 4/200\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.6458 - acc: 0.7925\n",
      "Epoch 5/200\n",
      "8000/8000 [==============================] - 0s 20us/step - loss: 0.6043 - acc: 0.7925\n",
      "Epoch 6/200\n",
      "8000/8000 [==============================] - 0s 19us/step - loss: 0.5497 - acc: 0.7925\n",
      "Epoch 7/200\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.4975 - acc: 0.7925\n",
      "Epoch 8/200\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.4658 - acc: 0.7925\n",
      "Epoch 9/200\n",
      "8000/8000 [==============================] - 0s 20us/step - loss: 0.4517 - acc: 0.7925\n",
      "Epoch 10/200\n",
      "8000/8000 [==============================] - 0s 20us/step - loss: 0.4463 - acc: 0.7925\n",
      "Epoch 11/200\n",
      "8000/8000 [==============================] - 0s 19us/step - loss: 0.4432 - acc: 0.7925\n",
      "Epoch 12/200\n",
      "8000/8000 [==============================] - 0s 18us/step - loss: 0.4410 - acc: 0.7925\n",
      "Epoch 13/200\n",
      "8000/8000 [==============================] - 0s 17us/step - loss: 0.4392 - acc: 0.7925\n",
      "Epoch 14/200\n",
      "8000/8000 [==============================] - 0s 18us/step - loss: 0.4377 - acc: 0.7925\n",
      "Epoch 15/200\n",
      "8000/8000 [==============================] - 0s 17us/step - loss: 0.4364 - acc: 0.7925\n",
      "Epoch 16/200\n",
      "8000/8000 [==============================] - 0s 17us/step - loss: 0.4353 - acc: 0.7925\n",
      "Epoch 17/200\n",
      "8000/8000 [==============================] - 0s 15us/step - loss: 0.4341 - acc: 0.7925\n",
      "Epoch 18/200\n",
      "8000/8000 [==============================] - 0s 19us/step - loss: 0.4330 - acc: 0.7925\n",
      "Epoch 19/200\n",
      "8000/8000 [==============================] - 0s 15us/step - loss: 0.4321 - acc: 0.7925\n",
      "Epoch 20/200\n",
      "8000/8000 [==============================] - 0s 15us/step - loss: 0.4311 - acc: 0.7925\n",
      "Epoch 21/200\n",
      "8000/8000 [==============================] - 0s 19us/step - loss: 0.4301 - acc: 0.7925: 0s - loss: 0.4250 - acc: 0.79\n",
      "Epoch 22/200\n",
      "8000/8000 [==============================] - 0s 15us/step - loss: 0.4291 - acc: 0.7925\n",
      "Epoch 23/200\n",
      "8000/8000 [==============================] - 0s 17us/step - loss: 0.4281 - acc: 0.8038\n",
      "Epoch 24/200\n",
      "8000/8000 [==============================] - 0s 19us/step - loss: 0.4269 - acc: 0.8093\n",
      "Epoch 25/200\n",
      "8000/8000 [==============================] - 0s 17us/step - loss: 0.4256 - acc: 0.8130\n",
      "Epoch 26/200\n",
      "8000/8000 [==============================] - 0s 17us/step - loss: 0.4243 - acc: 0.8161\n",
      "Epoch 27/200\n",
      "8000/8000 [==============================] - 0s 15us/step - loss: 0.4230 - acc: 0.8186\n",
      "Epoch 28/200\n",
      "8000/8000 [==============================] - 0s 13us/step - loss: 0.4216 - acc: 0.8207\n",
      "Epoch 29/200\n",
      "8000/8000 [==============================] - 0s 17us/step - loss: 0.4202 - acc: 0.8234\n",
      "Epoch 30/200\n",
      "8000/8000 [==============================] - 0s 16us/step - loss: 0.4189 - acc: 0.8261\n",
      "Epoch 31/200\n",
      "8000/8000 [==============================] - 0s 18us/step - loss: 0.4174 - acc: 0.8267\n",
      "Epoch 32/200\n",
      "8000/8000 [==============================] - 0s 14us/step - loss: 0.4161 - acc: 0.8294\n",
      "Epoch 33/200\n",
      "8000/8000 [==============================] - 0s 18us/step - loss: 0.4148 - acc: 0.8306\n",
      "Epoch 34/200\n",
      "8000/8000 [==============================] - 0s 17us/step - loss: 0.4135 - acc: 0.8318\n",
      "Epoch 35/200\n",
      "8000/8000 [==============================] - 0s 13us/step - loss: 0.4122 - acc: 0.8322\n",
      "Epoch 36/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.4108 - acc: 0.8329\n",
      "Epoch 37/200\n",
      "8000/8000 [==============================] - 0s 12us/step - loss: 0.4096 - acc: 0.8331\n",
      "Epoch 38/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.4085 - acc: 0.8335\n",
      "Epoch 39/200\n",
      "8000/8000 [==============================] - 0s 16us/step - loss: 0.4074 - acc: 0.8341\n",
      "Epoch 40/200\n",
      "8000/8000 [==============================] - 0s 16us/step - loss: 0.4065 - acc: 0.8340\n",
      "Epoch 41/200\n",
      "8000/8000 [==============================] - 0s 14us/step - loss: 0.4056 - acc: 0.8341\n",
      "Epoch 42/200\n",
      "8000/8000 [==============================] - 0s 13us/step - loss: 0.4046 - acc: 0.8362\n",
      "Epoch 43/200\n",
      "8000/8000 [==============================] - 0s 14us/step - loss: 0.4037 - acc: 0.8364: 0s - loss: 0.4051 - acc: 0.833\n",
      "Epoch 44/200\n",
      "8000/8000 [==============================] - 0s 12us/step - loss: 0.4029 - acc: 0.8364\n",
      "Epoch 45/200\n",
      "8000/8000 [==============================] - 0s 17us/step - loss: 0.4023 - acc: 0.8370\n",
      "Epoch 46/200\n",
      "8000/8000 [==============================] - 0s 15us/step - loss: 0.4013 - acc: 0.8368\n",
      "Epoch 47/200\n",
      "8000/8000 [==============================] - 0s 11us/step - loss: 0.4007 - acc: 0.8365\n",
      "Epoch 48/200\n",
      "8000/8000 [==============================] - 0s 12us/step - loss: 0.4000 - acc: 0.8372\n",
      "Epoch 49/200\n",
      "8000/8000 [==============================] - 0s 11us/step - loss: 0.3994 - acc: 0.8371\n",
      "Epoch 50/200\n",
      "8000/8000 [==============================] - 0s 11us/step - loss: 0.3988 - acc: 0.8380\n",
      "Epoch 51/200\n",
      "8000/8000 [==============================] - 0s 12us/step - loss: 0.3982 - acc: 0.8376\n",
      "Epoch 52/200\n",
      "8000/8000 [==============================] - 0s 11us/step - loss: 0.3977 - acc: 0.8385\n",
      "Epoch 53/200\n",
      "8000/8000 [==============================] - 0s 11us/step - loss: 0.3973 - acc: 0.8389\n",
      "Epoch 54/200\n",
      "8000/8000 [==============================] - 0s 12us/step - loss: 0.3969 - acc: 0.8370\n",
      "Epoch 55/200\n",
      "8000/8000 [==============================] - 0s 14us/step - loss: 0.3966 - acc: 0.8389\n",
      "Epoch 56/200\n",
      "8000/8000 [==============================] - 0s 12us/step - loss: 0.3961 - acc: 0.8386\n",
      "Epoch 57/200\n",
      "8000/8000 [==============================] - 0s 11us/step - loss: 0.3954 - acc: 0.8391\n",
      "Epoch 58/200\n",
      "8000/8000 [==============================] - 0s 11us/step - loss: 0.3951 - acc: 0.8391\n",
      "Epoch 59/200\n",
      "8000/8000 [==============================] - 0s 11us/step - loss: 0.3948 - acc: 0.8395\n",
      "Epoch 60/200\n",
      "8000/8000 [==============================] - 0s 13us/step - loss: 0.3943 - acc: 0.8381\n",
      "Epoch 61/200\n",
      "8000/8000 [==============================] - 0s 12us/step - loss: 0.3939 - acc: 0.8380\n",
      "Epoch 62/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3935 - acc: 0.8384\n",
      "Epoch 63/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3931 - acc: 0.8386\n",
      "Epoch 64/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3928 - acc: 0.8388\n",
      "Epoch 65/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3927 - acc: 0.8391\n",
      "Epoch 66/200\n",
      "8000/8000 [==============================] - 0s 13us/step - loss: 0.3923 - acc: 0.8390\n",
      "Epoch 67/200\n",
      "8000/8000 [==============================] - 0s 12us/step - loss: 0.3919 - acc: 0.8386\n",
      "Epoch 68/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3914 - acc: 0.8405\n",
      "Epoch 69/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3909 - acc: 0.8405\n",
      "Epoch 70/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3904 - acc: 0.8401\n",
      "Epoch 71/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3900 - acc: 0.8400\n",
      "Epoch 72/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3897 - acc: 0.8400\n",
      "Epoch 73/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3892 - acc: 0.8401\n",
      "Epoch 74/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3889 - acc: 0.8412\n",
      "Epoch 75/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3881 - acc: 0.8415\n",
      "Epoch 76/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3872 - acc: 0.8410\n",
      "Epoch 77/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3863 - acc: 0.8416\n",
      "Epoch 78/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3854 - acc: 0.8423\n",
      "Epoch 79/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3844 - acc: 0.8419\n",
      "Epoch 80/200\n",
      "8000/8000 [==============================] - 0s 11us/step - loss: 0.3833 - acc: 0.8416\n",
      "Epoch 81/200\n",
      "8000/8000 [==============================] - 0s 11us/step - loss: 0.3820 - acc: 0.8425\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3805 - acc: 0.8440\n",
      "Epoch 83/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3789 - acc: 0.8430\n",
      "Epoch 84/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3773 - acc: 0.8434\n",
      "Epoch 85/200\n",
      "8000/8000 [==============================] - 0s 12us/step - loss: 0.3756 - acc: 0.8443\n",
      "Epoch 86/200\n",
      "8000/8000 [==============================] - 0s 12us/step - loss: 0.3744 - acc: 0.8448\n",
      "Epoch 87/200\n",
      "8000/8000 [==============================] - 0s 12us/step - loss: 0.3712 - acc: 0.8454\n",
      "Epoch 88/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3695 - acc: 0.8458\n",
      "Epoch 89/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3672 - acc: 0.8469\n",
      "Epoch 90/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3653 - acc: 0.8474\n",
      "Epoch 91/200\n",
      "8000/8000 [==============================] - 0s 12us/step - loss: 0.3630 - acc: 0.8495\n",
      "Epoch 92/200\n",
      "8000/8000 [==============================] - 0s 13us/step - loss: 0.3603 - acc: 0.8500\n",
      "Epoch 93/200\n",
      "8000/8000 [==============================] - 0s 12us/step - loss: 0.3585 - acc: 0.8505\n",
      "Epoch 94/200\n",
      "8000/8000 [==============================] - 0s 12us/step - loss: 0.3575 - acc: 0.8530\n",
      "Epoch 95/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3555 - acc: 0.8530\n",
      "Epoch 96/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3531 - acc: 0.8529\n",
      "Epoch 97/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3517 - acc: 0.8538\n",
      "Epoch 98/200\n",
      "8000/8000 [==============================] - 0s 12us/step - loss: 0.3505 - acc: 0.8549\n",
      "Epoch 99/200\n",
      "8000/8000 [==============================] - 0s 11us/step - loss: 0.3492 - acc: 0.8551\n",
      "Epoch 100/200\n",
      "8000/8000 [==============================] - 0s 11us/step - loss: 0.3475 - acc: 0.8561\n",
      "Epoch 101/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3471 - acc: 0.8562\n",
      "Epoch 102/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3462 - acc: 0.8556\n",
      "Epoch 103/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3452 - acc: 0.8560\n",
      "Epoch 104/200\n",
      "8000/8000 [==============================] - 0s 11us/step - loss: 0.3441 - acc: 0.8554\n",
      "Epoch 105/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3441 - acc: 0.8566\n",
      "Epoch 106/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3426 - acc: 0.8572\n",
      "Epoch 107/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3423 - acc: 0.8570\n",
      "Epoch 108/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3415 - acc: 0.8587\n",
      "Epoch 109/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3406 - acc: 0.8591\n",
      "Epoch 110/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3397 - acc: 0.8598\n",
      "Epoch 111/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3398 - acc: 0.8597\n",
      "Epoch 112/200\n",
      "8000/8000 [==============================] - 0s 11us/step - loss: 0.3396 - acc: 0.8584\n",
      "Epoch 113/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3384 - acc: 0.8580\n",
      "Epoch 114/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3381 - acc: 0.8601\n",
      "Epoch 115/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3379 - acc: 0.8600\n",
      "Epoch 116/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3374 - acc: 0.8580\n",
      "Epoch 117/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3366 - acc: 0.8604\n",
      "Epoch 118/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3365 - acc: 0.8596\n",
      "Epoch 119/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3362 - acc: 0.8589\n",
      "Epoch 120/200\n",
      "8000/8000 [==============================] - 0s 10us/step - loss: 0.3362 - acc: 0.8606\n",
      "Epoch 121/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3362 - acc: 0.8600\n",
      "Epoch 122/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3356 - acc: 0.8601\n",
      "Epoch 123/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3347 - acc: 0.8623\n",
      "Epoch 124/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3345 - acc: 0.8611\n",
      "Epoch 125/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3348 - acc: 0.8617\n",
      "Epoch 126/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3341 - acc: 0.8597\n",
      "Epoch 127/200\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.3336 - acc: 0.8610\n",
      "Epoch 128/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3335 - acc: 0.8620\n",
      "Epoch 129/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3327 - acc: 0.8630\n",
      "Epoch 130/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3337 - acc: 0.8616\n",
      "Epoch 131/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3326 - acc: 0.8630\n",
      "Epoch 132/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3325 - acc: 0.8626\n",
      "Epoch 133/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3320 - acc: 0.8625\n",
      "Epoch 134/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3315 - acc: 0.8624\n",
      "Epoch 135/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3318 - acc: 0.8631\n",
      "Epoch 136/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3315 - acc: 0.8634\n",
      "Epoch 137/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3310 - acc: 0.8636\n",
      "Epoch 138/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3306 - acc: 0.8634\n",
      "Epoch 139/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3313 - acc: 0.8648\n",
      "Epoch 140/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3307 - acc: 0.8632\n",
      "Epoch 141/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3310 - acc: 0.8635\n",
      "Epoch 142/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3301 - acc: 0.8641\n",
      "Epoch 143/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3300 - acc: 0.8644\n",
      "Epoch 144/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3298 - acc: 0.8651\n",
      "Epoch 145/200\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.3305 - acc: 0.8644\n",
      "Epoch 146/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3305 - acc: 0.8637\n",
      "Epoch 147/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3295 - acc: 0.8645\n",
      "Epoch 148/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3304 - acc: 0.8642\n",
      "Epoch 149/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3293 - acc: 0.8649\n",
      "Epoch 150/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3295 - acc: 0.8641\n",
      "Epoch 151/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3294 - acc: 0.8665\n",
      "Epoch 152/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3295 - acc: 0.8640\n",
      "Epoch 153/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3298 - acc: 0.8657\n",
      "Epoch 154/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3287 - acc: 0.8648\n",
      "Epoch 155/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3284 - acc: 0.8659\n",
      "Epoch 156/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3280 - acc: 0.8658\n",
      "Epoch 157/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3282 - acc: 0.8656\n",
      "Epoch 158/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3280 - acc: 0.8669\n",
      "Epoch 159/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3279 - acc: 0.8656\n",
      "Epoch 160/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3279 - acc: 0.8666\n",
      "Epoch 161/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3288 - acc: 0.8644\n",
      "Epoch 162/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3275 - acc: 0.8674\n",
      "Epoch 163/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3275 - acc: 0.8669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3272 - acc: 0.8647\n",
      "Epoch 165/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3274 - acc: 0.8664\n",
      "Epoch 166/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3275 - acc: 0.8654\n",
      "Epoch 167/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3275 - acc: 0.8661\n",
      "Epoch 168/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3269 - acc: 0.8665\n",
      "Epoch 169/200\n",
      "8000/8000 [==============================] - 0s 6us/step - loss: 0.3267 - acc: 0.8659\n",
      "Epoch 170/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3264 - acc: 0.8661\n",
      "Epoch 171/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3267 - acc: 0.8654\n",
      "Epoch 172/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3268 - acc: 0.8664A: 0s - loss: 0.3294 - acc: 0.865\n",
      "Epoch 173/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3271 - acc: 0.8652\n",
      "Epoch 174/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3274 - acc: 0.8645\n",
      "Epoch 175/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3262 - acc: 0.8648\n",
      "Epoch 176/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3263 - acc: 0.8664\n",
      "Epoch 177/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3259 - acc: 0.8666\n",
      "Epoch 178/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3259 - acc: 0.8664\n",
      "Epoch 179/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3258 - acc: 0.8666\n",
      "Epoch 180/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3260 - acc: 0.8664\n",
      "Epoch 181/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3258 - acc: 0.8662\n",
      "Epoch 182/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3256 - acc: 0.8657\n",
      "Epoch 183/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3257 - acc: 0.8663\n",
      "Epoch 184/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3259 - acc: 0.8661\n",
      "Epoch 185/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3263 - acc: 0.8650\n",
      "Epoch 186/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3253 - acc: 0.8662\n",
      "Epoch 187/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3251 - acc: 0.8666\n",
      "Epoch 188/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3257 - acc: 0.8651\n",
      "Epoch 189/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3251 - acc: 0.8670\n",
      "Epoch 190/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3252 - acc: 0.8667\n",
      "Epoch 191/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3255 - acc: 0.8665\n",
      "Epoch 192/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3257 - acc: 0.8670\n",
      "Epoch 193/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3248 - acc: 0.8661\n",
      "Epoch 194/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3248 - acc: 0.8666\n",
      "Epoch 195/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3248 - acc: 0.8676\n",
      "Epoch 196/200\n",
      "8000/8000 [==============================] - 0s 8us/step - loss: 0.3246 - acc: 0.8666\n",
      "Epoch 197/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3252 - acc: 0.8675\n",
      "Epoch 198/200\n",
      "8000/8000 [==============================] - 0s 7us/step - loss: 0.3258 - acc: 0.8676\n",
      "Epoch 199/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3251 - acc: 0.8670\n",
      "Epoch 200/200\n",
      "8000/8000 [==============================] - 0s 9us/step - loss: 0.3250 - acc: 0.8663\n",
      "Test set Accuracy : 85.75%\n"
     ]
    }
   ],
   "source": [
    "# Comparing our results with the library keras.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "X_train, X_test,y_train,y_test = X_train.T, X_test.T,y_train.T, y_test.T\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(input_dim=11, units = 16, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
    "classifier.add(Dense(units = 16, kernel_initializer = \"uniform\", activation=\"relu\"))\n",
    "# classifier.add(Dense(units = 16, kernel_initializer = \"uniform\", activation=\"tanh\"))\n",
    "classifier.add(Dense(units = 1, kernel_initializer = \"uniform\", activation = \"sigmoid\"))\n",
    "classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "classifier.fit(X_train, y_train, batch_size = 500, epochs = 200)\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = 1*(y_pred > 0.5)\n",
    "test_acc = sum(sum(y_pred == y_test)) / y_test.size\n",
    "print(f\"Test set Accuracy : {test_acc*100}%\")\n",
    "X_train, X_test,y_train,y_test = X_train.T, X_test.T,y_train.T, y_test.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
