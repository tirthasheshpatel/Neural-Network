{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Modelling with Neural Networks!#\n",
    "<p>In this notebook, I have made a Artificial Neural Network from scratch and used it for churn modelling. Every detail of a neural network is explained with a tutorial on how to perform churn modelling \"<i>correctly</i>\". Churn modelling is a very sofisticated task and needs a lot of attention on choosing the metric for optimization. A very common mistake that practitioners do is choosing the accuracy as optimizing metric. It is explained in this notebook which metric is best for churn modelling and which machine learning architectures to use for the purpose. </p>\n",
    "<p> I have used the dataset openly available on SuperDataScience website. It contains data of 10,000 customers (of course, it's synthetic). I have performed EDA and Feature Engineering openly available on Tableau Public Website. The details of data cleaning step is given in the Data Science Notebook.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing the Libraries ###\n",
    "<p> We will use numpy for doing matrix algebra, matplotlib to plot graphs(of course!), pandas to read our dataset, sklearn to clean and preprocess our dataset. warnings, time, os, and sys have been used rarely. ProcessPoolExecutor is used to perform parallelism and train multiple models on seperate CPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from concurrent.futures import ProcessPoolExecutor as ppe\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\n",
    "# Setting up the environment\n",
    "warnings.filterwarnings('ignore')\n",
    "os.system(\"CLS\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Importing our Dataset.###\n",
    "<p> We have imported the dataset and split it into a training, test and dev sets. Further, we have removed unwanted features and normalized the remaining features. Categorical features have been OneHot Encoded.</p>\n",
    "<p><strong>Notice</strong>: We have taken the transpose of training, test, and dev sets. It is because, we want the datasets to have shape (m,n) where m are the number of examples and n are the number of features. Most of the open source libs use the (n,m) shape but for our simplicity, the shapes have been altered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting our dataset\n",
    "os.chdir(\"C:\\\\Users\\\\Hilak\\\\Desktop\\\\INTERESTS\\\\Machine Learning A-Z Template Folder\\\\Part 8 - Deep Learning\\\\Section 39 - Artificial Neural Networks (ANN)\");\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, [13]].values\n",
    "\n",
    "# Encoding categorical data\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "X_test, X_CV, y_test, y_CV = train_test_split(X_test, y_test, test_size = 0.5)\n",
    "\n",
    "# Feature Scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "X_CV = sc.transform(X_CV)\n",
    "\n",
    "# from sklearn.decomposition import KernelPCA\n",
    "# kpca = KernelPCA(n_components = 2, kernel=\"rbf\")\n",
    "# X_train = kpca.fit_transform(X_train)\n",
    "# X_test = kpca.transform(X_test)\n",
    "\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T\n",
    "X_CV = X_CV.T\n",
    "y_CV = y_CV.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement : This dataset contains the data of customers of a bank. The bank has hired you as a data scientist to predict if the customer will stay with the bank or leave. ###\n",
    "<p><li>Total no. of examples in the dataset     : 10000</li>\n",
    "<li>No. of examples in the training set      : 8000</li>\n",
    "<li>No. of examples in the test set          : 1000</li>\n",
    "<li>No. of examples in the CV set            : 1000</li></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Our Neural Network ##\n",
    "<p> Now, let's proceed to make our Artificial Neural Network and some more interesting stuff for our modelling.</p>\n",
    "<p> To understand the neural networks is a pretty hard job and only coding them clears most of the subtle concepts that we may have missed or ignored during reading about them or watching a tutorial or a course video. I have written a comment in every line explaining each of them. I have written an <a href=\"#\">article on this implementation</a> on Medium if you are interested having a look at it.</p>\n",
    "<p> Let me explain in brief, what features have I implemented in this code.</p>\n",
    "<ol>\n",
    "    <li><p><i>NeuralNet(layers, X, y, ac_funcs, init_method, loss_func)</i> to initialize our network.</p></li>\n",
    "    <li><p><i>NeuralNet.startTraining(batch_size, epochs, alpha, decay_rate, _lambda, keep_prob, threshold, beta1, beta2, interval, print_metrics, evaluate, X_test, y_test)</i> to start training our network.</p></li>\n",
    "    <li><p><i>NeuralNet.predict(X_test)</i> to predict on a test set.</p></li>\n",
    "    <li><p><i>NeuralNet.plot_decision_boundary(X_set, y_set, title, xlab, ylab)</i> to plot the dicision boundary in case you have only two features. Note: The dataset must have two features.</p></li>\n",
    "</ol>\n",
    "<p>I have also implemented HyperParameterTuning techniques like GridSearch and RandomizedGridSearch. You can go ahead and take a look at them by yourself as there are no comments to explain it but, don't worry, it's pretty straight-forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z) :\n",
    "    \"\"\" Reutrns the element wise sigmoid function. \"\"\"\n",
    "    return 1./(1 + np.exp(-z))\n",
    "def sigmoid_prime(z) :\n",
    "    \"\"\" Returns the derivative of the sigmoid function. \"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "def ReLU(z) : \n",
    "    \"\"\" Reutrns the element wise ReLU function. \"\"\"\n",
    "    return (z*(z > 0))\n",
    "def ReLU_prime(z) :\n",
    "    \"\"\" Returns the derivative of the ReLU function. \"\"\"\n",
    "    return 1*(z>=0)\n",
    "def lReLU(z) : \n",
    "    \"\"\" Reutrns the element wise leaky ReLU function. \"\"\"\n",
    "    return np.maximum(z/100,z)\n",
    "def lReLU_prime(z) :\n",
    "    \"\"\" Returns the derivative of the leaky ReLU function. \"\"\"\n",
    "    z = 1*(z>=0)\n",
    "    z[z==0] = 1/100\n",
    "    return z\n",
    "def tanh(z) :\n",
    "    \"\"\" Reutrns the element wise hyperbolic tangent function. \"\"\"\n",
    "    return np.tanh(z)\n",
    "def tanh_prime(z) : \n",
    "    \"\"\" Returns the derivative of the tanh function. \"\"\"\n",
    "    return (1-tanh(z)**2)\n",
    "def softmax(z) :\n",
    "    t = np.exp(z)\n",
    "    return (t/np.sum(t,axis=0))\n",
    "def softmax_prime(z):\n",
    "    return softmax(z)*(1-softmax(z))\n",
    "    \n",
    "\n",
    "# A dictionary of our activation functions\n",
    "PHI = {'sigmoid':sigmoid, 'relu':ReLU, 'lrelu':lReLU, 'tanh':tanh, 'softmax':softmax}\n",
    "\n",
    "# A dictionary containing the derivatives of our activation functions\n",
    "PHI_PRIME = {'sigmoid':sigmoid_prime, 'relu':ReLU_prime, 'lrelu':lReLU_prime, 'tanh':tanh_prime, 'softmax':softmax_prime}\n",
    "\n",
    "\n",
    "class NeuralNet : \n",
    "    \"\"\"\n",
    "    This is a class for implementing Artificial Neural Networks. L2 and Droupout are the \n",
    "    default regularization methods implemented in this class. It takes the following parameters:\n",
    "    \n",
    "    1. layers      : A python list containing the different number of neurons in each layer.\n",
    "                     (containing the output layer)\n",
    "                     Eg - [64,32,16,16,1]\n",
    "                \n",
    "    2. X           : Matrix of features with rows as features and columns as different examples.\n",
    "    \n",
    "    3. y           : Numpy array containing the ouputs of coresponding examples.\n",
    "    \n",
    "    4. ac_funcs    : A python list containing activation function of each layer.\n",
    "                     Eg - ['relu','relu','lrelu','tanh','sigmoid']\n",
    "    \n",
    "    5. init_method : Meathod to initialize weights of the network. Can be 'gaussian','random','zeros'.\n",
    "    \n",
    "    6. loss_func   : Currently not implemented\n",
    "    \n",
    "    7. W           : Weights of a pretrained neural network with same architecture.\n",
    "    \n",
    "    8. B           : Biases of a pretrained neural network with same architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, X, y, ac_funcs, init_method='xavier', loss_func='b_ce', W=np.array([]), B=np.array([])) :\n",
    "        \"\"\" Initialize the network. \"\"\"\n",
    "        # Store the layers of the network\n",
    "        self.layers = layers\n",
    "        # ----\n",
    "        self.W = None\n",
    "        self.B = None\n",
    "        # Store the number of examples in the dataset as m\n",
    "        self.m = X.shape[1]\n",
    "        # Store the full layer list as n\n",
    "        self.n = [X.shape[0], *layers]\n",
    "        # Save the dataset\n",
    "        self.X = X\n",
    "        # Save coresponding output\n",
    "        self.y = y\n",
    "        self.X_mini = None\n",
    "        self.y_mini = None\n",
    "        self.m_mini = None\n",
    "        # List to store the cost of the model calculated during training\n",
    "        self.cost = []\n",
    "        # Stores the accuracy obtained on the test set.\n",
    "        self.acc = 0\n",
    "        # Activation function of each layer\n",
    "        self.ac_funcs = ac_funcs\n",
    "        self.loss = loss_func\n",
    "        # Initialize the weights by provided method.\n",
    "        if len(W) and len(B) :\n",
    "            self.W = W\n",
    "            self.B = B\n",
    "        else : \n",
    "            if init_method=='xavier': \n",
    "                self.W = [np.random.randn(self.n[nl], self.n[nl-1])*np.sqrt(2/self.n[nl-1]) for nl in range(1,len(self.n))]\n",
    "                self.B = [np.zeros((nl,1), 'float32') for nl in self.layers]\n",
    "            elif init_method=='gaussian': \n",
    "                self.W = [np.random.randn(self.n[nl], self.n[nl-1]) for nl in range(1,len(self.n))]\n",
    "                self.B = [np.zeros((nl,1), 'float32') for nl in self.layers]\n",
    "            elif init_method == 'random':\n",
    "                self.W = [np.random.rand(self.n[nl], self.n[nl-1]) for nl in range(1,len(self.n))]\n",
    "                self.B = [np.random.rand(nl,1) for nl in self.layers]\n",
    "            elif init_method == 'zeros':\n",
    "                self.W = [np.zeros((self.n[nl], self.n[nl-1]), 'float32') for nl in range(1,len(self.n))]\n",
    "                self.B = [np.zeros((nl,1), 'float32') for nl in self.layers]\n",
    "        self.vdw = [np.zeros(i.shape) for i in self.W]\n",
    "        self.vdb = [np.zeros(i.shape) for i in self.B]\n",
    "        self.sdw = [np.zeros(i.shape) for i in self.W]\n",
    "        self.sdb = [np.zeros(i.shape) for i in self.B]\n",
    "    \n",
    "    def startTraining(self, batch_size, epochs, alpha, decay_rate, _lambda, keep_prob, threshold=0.5, beta1=0.9, beta2=0.999, interval=10, print_metrics = True, evaluate=False, X_test=None, y_test=None):\n",
    "        \"\"\"\n",
    "        Start training the neural network. It takes the followng parameters : \n",
    "        \n",
    "        1. batch_size : Size of your mini batch. Must be greater than 1.\n",
    "        \n",
    "        2. epochs         : Number of epochs for which you want to train the network.\n",
    "        \n",
    "        3. alpha          : The learning rate of your network.\n",
    "        \n",
    "        4. decay_rate     : The rate at which you want to decrease your learning rate.\n",
    "\n",
    "        5. _lambda        : L2 regularization parameter or the penalization parameter.\n",
    "\n",
    "        6. keep_prob      : Python List. Dropout regularization parameter. The percentage of neurons to keep activated.\n",
    "                            Eg - 0.8 means 20% of the neurons have been deactivated.\n",
    "\n",
    "        7. threshold      : Threshold for binary classification\n",
    "        \n",
    "        8. beta1          : Momentum. default=0.9\n",
    "        \n",
    "        9. beta2          : RMSprop Parameter. default=0.999\n",
    "        \n",
    "        10. interval      : The interval between updates of cost and accuracy. default=10\n",
    "\n",
    "        11. print_metrics : Boolean. Controls printing of metrics. default=True\n",
    "\n",
    "        12. evaluate      : Boolean. True if you want to evaluate your model on a test set. default=False\n",
    "\n",
    "        13. X_test        : Test set to be provided if evaluate=True.\n",
    "\n",
    "        14. y_test        : Labels coresponding to the test set. \n",
    "        \"\"\"\n",
    "        dataset_size = self.X.shape[1]                                                                                                                                                  # Store the dataset size.\n",
    "        k=1                                                                                                                                                                             # Step.\n",
    "        cost_val = 0                                                                                                                                                                    # Variable to store the value of cost while we are training our network.\n",
    "        for j in range(1, epochs+1):                                                                                                                                                    # Looping epochs no. of times.\n",
    "            start = time.time()                                                                                                                                                         # Starting to train a mini_batch.\n",
    "            alpha = alpha/( 1 + decay_rate*(j-1) )                                                                                                                                      # Decaying learning rate after every epoch.\n",
    "            for i in range(0, dataset_size-batch_size+1, batch_size):                                                                                                                   # Loop to devide the dataset into mini_batches and training each of them.\n",
    "                self.X_mini = self.X[:, i:i+batch_size]                                                                                                                                 # Slicing dataset into one mini_batch.\n",
    "                self.y_mini = self.y[:, i:i+batch_size]                                                                                                                                 # Slicing coresponding labels.\n",
    "                self.m_mini = self.y_mini.shape[1]                                                                                                                                      # Storing the number of examples in the mini_batch. Used later during calculation of cost derivative.\n",
    "                self._miniBatchTraining(alpha, beta1, beta2, _lambda, keep_prob,k)                                                                                                      # Training a single mini_batch.\n",
    "                if not k%interval:                                                                                                                                                      # Print metrics after every interval.\n",
    "                    aa = self.predict(self.X)                                                                                                                                           # Predicting on our dataset.\n",
    "                    if self.loss == 'b_ce':                                                                                                                                             # Handing metrics for binary classification.\n",
    "                        aa_ = aa > threshold                                                                                                                                            # Thresholding.\n",
    "                        self.acc = np.sum(aa_ == self.y) / self.m                                                                                                                       # Calculating the accuracy achieved on the training_set.\n",
    "                        cost_val = self._cost_func(aa, _lambda)                                                                                                                         # Calculating the value of cost function.\n",
    "                        self.cost.append(cost_val)                                                                                                                                      # Logging it in the list of cost values.\n",
    "                    elif self.loss == 'c_ce':                                                                                                                                           # Haldling metrics for categorical classification.\n",
    "                        aa_ = np.argmax(aa, axis = 0)                                                                                                                                   # Getting the label of our prediction.\n",
    "                        yy = np.argmax(self.y, axis = 0)                                                                                                                                # Getting the correct label.\n",
    "                        self.acc = np.sum(aa_==yy)/(self.m)                                                                                                                             # Comparing our predictions with correct labels.\n",
    "                        cost_val = self._cost_func(aa, _lambda)                                                                                                                         # Calculating the value of cost function.\n",
    "                        self.cost.append(cost_val)                                                                                                                                      # Logging it in the list of cost values.\n",
    "                if print_metrics:                                                                                                                                                       # If the user wants to print the metrics, print them.\n",
    "                    sys.stdout.write(f'\\rEpoch[{j}] {i+batch_size}/{self.m} : Cost = {cost_val:.4f} ; Acc = {(self.acc*100):.2f}% ; Time Taken = {(time.time()-start):.0f}s')           # Print the metrics.\n",
    "                k+=1                                                                                                                                                                    # Increment the step by one.\n",
    "            print('\\n')                                                                                                                                                                 # Line Break.\n",
    "        if evaluate:                                                                                                                                                                    # Checking if the user wants to evaluate the model on a test set.\n",
    "            print(f\"For batch_size = {batch_size}, epochs = {epochs}, alpha = {alpha}, decay_rate = {decay_rate}, _lambda = {_lambda}, keep_prob = {keep_prob}\")                        # Print the values of hyperparameters.\n",
    "            aa = self.predict(X_test)                                                                                                                                                   # Predicting the labels of test set.\n",
    "            if self.loss == 'b_ce':                                                                                                                                                     # Handling the metrics for binary classification.\n",
    "                aa_ = aa > threshold                                                                                                                                                    # Thresholding.\n",
    "                acc = np.sum(aa_ == y_test) / X_test.shape[1]                                                                                                                           # Calculating the accuracy achieved on the test set.\n",
    "                print(f\"Accuracy on training set: {self.acc}\")                                                                                                                          # Print the accuracy achieved on the training_set.\n",
    "                print(f\"Accuracy on test set: {acc}\\n\")                                                                                                                                 # Print the accuracy achieved on the test set.\n",
    "            elif self.loss == 'c_ce':                                                                                                                                                   # Handling the metrics for categorical classification.\n",
    "                aa_ = np.argmax(aa, axis = 0)                                                                                                                                           # Getting the label of our prediction.\n",
    "                yy = np.argmax(y_test, axis = 0)                                                                                                                                        # Getting the correct label.\n",
    "                acc = np.sum(aa_==yy)/(X_test.shape[1])                                                                                                                                 # Comparing our predictions with correct labels.\n",
    "                print(f\"Accuracy on training set: {self.acc}\")                                                                                                                          # Print the accuracy achieved on the training_set.\n",
    "                print(f\"Accuracy on test set: {acc}\\n\")                                                                                                                                 # Print the accuracy achieved on the test set.\n",
    "        \n",
    "    \n",
    "    def _miniBatchTraining(self, alpha, beta1, beta2, _lambda, keep_prob,i):\n",
    "        epsilon = 1e-8\n",
    "        z,a = self._feedForward(keep_prob)\n",
    "        delta = self._cost_derivative(a[-1])\n",
    "        for l in range(1,len(z)) : \n",
    "            delta_w = (1/self.m_mini)*(np.dot(delta, a[-l-1].T) + (_lambda)*self.W[-l])\n",
    "            delta_b = (1/self.m_mini)*(np.sum(delta, axis=1, keepdims=True))\n",
    "            self.vdw[-l] = (beta1*self.vdw[-l] + (1-beta1)*delta_w)\n",
    "            vdw_corrected = self.vdw[-l]/(1 - beta1**i)\n",
    "            self.vdb[-l] = (beta1*self.vdb[-l] + (1-beta1)*delta_b)\n",
    "            vdb_corrected = self.vdb[-l]/(1 - beta1**i)\n",
    "            self.sdw[-l] = (beta2*self.sdw[-l] + (1-beta2)*(delta_w**2))\n",
    "            sdw_corrected = self.sdw[-l]/(1 - beta2**i)\n",
    "            self.sdb[-l] = (beta2*self.sdb[-l] + (1-beta2)*(delta_b**2))\n",
    "            sdb_corrected = self.sdb[-l]/(1 - beta2**i)\n",
    "            delta = np.dot(self.W[-l].T, delta)*PHI_PRIME[self.ac_funcs[-l-1]](z[-l-1])\n",
    "            self.W[-l] = self.W[-l] - (alpha)*(vdw_corrected/(np.sqrt(sdw_corrected)+epsilon))\n",
    "            self.B[-l] = self.B[-l] - (alpha)*(vdb_corrected/(np.sqrt(sdb_corrected)+epsilon))\n",
    "        delta_w = (1/self.m_mini)*(np.dot(delta, self.X_mini.T ) + (_lambda)*self.W[0])\n",
    "        delta_b = (1/self.m_mini)*(np.sum(delta, axis=1, keepdims=True))\n",
    "        self.vdw[0] = (beta1*self.vdw[0] + (1-beta1)*delta_w)\n",
    "        vdw_corrected = self.vdw[0]/(1 - beta1**i)\n",
    "        self.vdb[0] = (beta1*self.vdb[0] + (1-beta1)*delta_b)\n",
    "        vdb_corrected = self.vdb[0]/(1 - beta1**i)\n",
    "        self.sdw[0] = (beta2*self.sdw[0] + (1-beta2)*(delta_w**2))\n",
    "        sdw_corrected = self.sdw[0]/(1 - beta2**i)\n",
    "        self.sdb[0] = (beta2*self.sdb[0] + (1-beta2)*(delta_b**2))\n",
    "        sdb_corrected = self.sdb[0]/(1 - beta2**i)\n",
    "        self.W[0] = self.W[0] - (alpha)*(vdw_corrected/(np.sqrt(sdw_corrected)+epsilon))\n",
    "        self.B[0] = self.B[0] - (alpha)*(vdb_corrected/(np.sqrt(sdb_corrected)+epsilon))\n",
    "        return None\n",
    "    \n",
    "    def predict(self, X_test) :\n",
    "        \"\"\" Predict the labels for a new dataset. Returns probability. \"\"\"\n",
    "        a = PHI[self.ac_funcs[0]](np.dot(self.W[0], X_test) + self.B[0])\n",
    "        for l in range(1,len(self.layers)):\n",
    "            a = PHI[self.ac_funcs[l]](np.dot(self.W[l], a) + self.B[l])\n",
    "        return a\n",
    "            \n",
    "    \n",
    "    def _feedForward(self, keep_prob):\n",
    "        \"\"\" Forward pass \"\"\"\n",
    "        z = [];a = []\n",
    "        z.append(np.dot(self.W[0], self.X_mini) + self.B[0])\n",
    "        a.append(PHI[self.ac_funcs[0]](z[-1]))\n",
    "        for l in range(1,len(self.layers)-1):\n",
    "            z.append(np.dot(self.W[l], a[-1]) + self.B[l])\n",
    "            # a.append(PHI[self.ac_funcs[l]](z[l]))\n",
    "            _a = PHI[self.ac_funcs[l]](z[l])\n",
    "            a.append( ((np.random.rand(*_a.shape) < keep_prob[l-1])*_a)/keep_prob[l-1] )\n",
    "        z.append(np.dot(self.W[-1], a[-1]) + self.B[-1])\n",
    "        a.append(PHI[self.ac_funcs[-1]](z[-1]))\n",
    "        return z,a\n",
    "    \n",
    "    def _cost_func(self, a, _lambda):\n",
    "        \"\"\" Binary Cross Entropy Cost Function \"\"\"\n",
    "        if self.ac_funcs[-1] == 'sigmoid':\n",
    "            return ( (-1/self.m)*np.sum(np.nan_to_num(self.y*np.log(a+10e-8) + (1-self.y)*np.log(1-a+10e-8))) + (_lambda/(2*self.m))*np.sum([np.sum(i**2) for i in self.W]) )\n",
    "        return ( (-1/self.m)*np.sum(np.nan_to_num(self.y*np.log(a))) + (_lambda/(2*self.m))*np.sum([np.sum(i**2) for i in self.W]) )\n",
    "        \n",
    "    def _cost_derivative(self, a) : \n",
    "        \"\"\" The derivative of cost w.r.t z \"\"\"\n",
    "        return (a-self.y_mini)\n",
    "\n",
    "    def plot_decision_boundary(self, X_set, y_set, title='', xlab='', ylab=''):\n",
    "        X_set, y_set = X_set.T, y_set.T\n",
    "        X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                             np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "        preds = self.predict(np.array([X1.ravel(), X2.ravel()])).reshape(X1.shape)\n",
    "        preds = 1*(preds > 0.5)\n",
    "        plt.contourf(X1, X2, preds, alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "        plt.xlim(X1.min(), X1.max())\n",
    "        plt.ylim(X2.min(), X2.max())\n",
    "        for i, j in enumerate(np.unique(y_set[:,0])):\n",
    "            plt.scatter(X_set[y_set[:,0] == j, 0], X_set[y_set[:,0] == j, 1],\n",
    "                        c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(xlab)\n",
    "        plt.ylabel(ylab)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "   \n",
    "    @property\n",
    "    def summary(self) :\n",
    "        return self.cost, self.acc, self.W,self.B\n",
    "    def __repr__(self) : \n",
    "        return f'<Neural Network at {id(self)}>'\n",
    "\n",
    "\n",
    "class HyperParameterTuning:\n",
    "\n",
    "    def __init__(self, layers, X, y, ac_funcs, X_test, y_test):\n",
    "        self.layers = layers\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.ac_funcs = ac_funcs\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "\n",
    "    def GridSearch(self, params):\n",
    "        if __name__ == '__main__':\n",
    "            models=[]\n",
    "            with ppe(max_workers = len(params)) as pool:\n",
    "                for param in params:\n",
    "                    models.append(NeuralNet(self.layers, self.X, self.y, self.ac_funcs))\n",
    "                    pool.submit(models[-1].startTraining, batch_size=param['batch_size'], epochs=param['epochs'], alpha=param['alpha'], decay_rate=param['decay_rate'], _lambda=param['_lambda'], keep_prob=param['keep_prob'], print_metrics=False, evaluate=True, X_test=self.X_test, y_test=self.y_test)\n",
    "        return models\n",
    "\n",
    "    def RandomizedGridSearch(self, params_range, nb_models):\n",
    "        if __name__ == '__main__':\n",
    "            models = []\n",
    "            params = []\n",
    "            for i in range(nb_models):\n",
    "                params.append({\n",
    "                    'batch_size' : int(np.round(np.random.rand()*(params_range['batch_size'][1]-params_range['batch_size'][0]) + params_range['batch_size'][0])),\n",
    "                    'epochs' : int(np.round(np.random.rand()*(params_range['epochs'][1]-params_range['epochs'][0]) + params_range['epochs'][0])),\n",
    "                    'alpha' : 10**(np.random.rand()*(np.log10(params_range['alpha'][1])-np.log10(params_range['alpha'][0])) + np.log10(params_range['alpha'][0])),\n",
    "                    'decay_rate' : 10**(np.random.rand()*(np.log10(params_range['decay_rate'][1])-np.log10(params_range['decay_rate'][0])) + np.log10(params_range['decay_rate'][0])),\n",
    "                    '_lambda' : 10**(np.random.rand()*(np.log10(params_range['_lambda'][1])-np.log10(params_range['_lambda'][0])) + np.log10(params_range['_lambda'][0])),\n",
    "                    'keep_prob' : [(np.random.rand()*(params_range['keep_prob'][1]-params_range['keep_prob'][0]) + params_range['keep_prob'][0]) for j in range(len(self.layers)-1)]\n",
    "                    })\n",
    "            with ppe(max_workers = len(params)) as pool:\n",
    "                for param in params:\n",
    "                    models.append(NeuralNet(self.layers, self.X, self.y, self.ac_funcs))\n",
    "                    pool.submit(models[-1].startTraining, batch_size=param['batch_size'], epochs=param['epochs'], alpha=param['alpha'], decay_rate=param['decay_rate'], _lambda=param['_lambda'], keep_prob=param['keep_prob'], print_metrics=False, evaluate=True, X_test=self.X_test, y_test=self.y_test)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'<HPT at {id(self)}>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Using our Neural Network for Churn Modelling###\n",
    "<p> Horray! We have implemented a fully vectorized Neural Network with Dropout and L2 Regularization with HyperParameterTuning as bonus. Now, we can finally use it for making all sorts of models for classification purposes. We know that, Neural Nets outperform most of the classification algorithms and is one of the most used algorithms for wide range of classification tasks. </p>\n",
    "<p> We are going to use this neural network for churn modelling which is a very sophisticated classification tasks and needs a lot of micro level understanding of the data to perform it successfully.</p>\n",
    "<p> We first initialize a network with two layers have 32 neurons and ReLU activation. After a lot of hyperparameter searching, I have came across 0.01 as the best learning rate with a batch_size of 500, penelization of 0.7, droprate of 90% in both the layers, and a decay_rate of 0.0001. We train our model for 100 epochs.</p>\n",
    "<p> A very important hyperparameter for our task is the <i>threshold</i>. It is the bound after which we predict that the customer will leave the bank. With the threshold of 0.5 (which most of the practitioners use), we have a preety good accuracy of >85% on train, test, and dev set. But, as I have mentioned above, accuracy is not the correct optimizing metric for our task. Let us understand why. First, we have a data that says if the customer has left or stayed with the bank. It is obvious that it would be skewed (meaning there would be more negatives than positives). This dataset is 79.60% skewed i.e. 79.60% of the customers have stayed with the bank. If we have a model that just spits out 0 for any and every example we pass in, we would still have a decent accuracy of 79.60%. This shows that >85% accuracy is not \"great\". This has been tested and very poor results have been obtained with <40% recall and <40% F1 score. This shows that we are predicting that the customer will not leave the bank while the customer acually left the bank. But, the bank wants to retain as many customers as possible. With such a high skewness, our model has a natural bias towards 0 (or the customer never leave the bank) hence leading the bank towards a huge loss. Moreover, predicting that a customer may leave even if he actully isn't doesn't cost the bank much. Hence, we need to have recall as the optimizing metric with accuracy and F1 score as satisficing metrics. This choice of metric may vary from task to task but bear in mind that accuracy is never a good metric for churn modelling. </p>\n",
    "<p> Now, to increse the recall, we need to set the threshold low enough such that it doesn't hurt the performance of the model much. Threshold has the control of how much \"confidence\" you need to say that the customer may leave the bank. A fair choice of confidence would be 0.5, meaning, predict that the customer leaves if we are above 50% sure. But, this arises the problem described above. Hence, by lowering the threshold, we are just saying that the customer would leave with a low confidence. This would lead to predicting that the customer would leave more often than saying that he would stay. This can lead to more false positives but it is fine for the bank as long as it retains most of its customers. </p>\n",
    "<p> Having said that, let's get our first model trained</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] 8000/8000 : Cost = 0.4867 ; Acc = 75.12% ; Time Taken = 0s\n",
      "\n",
      "Epoch[2] 8000/8000 : Cost = 0.4212 ; Acc = 75.56% ; Time Taken = 0s\n",
      "\n",
      "Epoch[3] 8000/8000 : Cost = 0.4042 ; Acc = 72.72% ; Time Taken = 0s\n",
      "\n",
      "Epoch[4] 8000/8000 : Cost = 0.3725 ; Acc = 77.35% ; Time Taken = 0s\n",
      "\n",
      "Epoch[5] 8000/8000 : Cost = 0.3497 ; Acc = 80.77% ; Time Taken = 0s\n",
      "\n",
      "Epoch[6] 8000/8000 : Cost = 0.3442 ; Acc = 81.12% ; Time Taken = 0s\n",
      "\n",
      "Epoch[7] 8000/8000 : Cost = 0.3384 ; Acc = 81.77% ; Time Taken = 0s\n",
      "\n",
      "Epoch[8] 8000/8000 : Cost = 0.3367 ; Acc = 81.96% ; Time Taken = 0s\n",
      "\n",
      "Epoch[9] 8000/8000 : Cost = 0.3334 ; Acc = 82.30% ; Time Taken = 0s\n",
      "\n",
      "Epoch[10] 8000/8000 : Cost = 0.3311 ; Acc = 82.09% ; Time Taken = 0s\n",
      "\n",
      "Epoch[11] 8000/8000 : Cost = 0.3296 ; Acc = 82.39% ; Time Taken = 0s\n",
      "\n",
      "Epoch[12] 8000/8000 : Cost = 0.3288 ; Acc = 82.60% ; Time Taken = 0s\n",
      "\n",
      "Epoch[13] 8000/8000 : Cost = 0.3276 ; Acc = 82.62% ; Time Taken = 0s\n",
      "\n",
      "Epoch[14] 8000/8000 : Cost = 0.3260 ; Acc = 83.05% ; Time Taken = 0s\n",
      "\n",
      "Epoch[15] 8000/8000 : Cost = 0.3255 ; Acc = 82.84% ; Time Taken = 0s\n",
      "\n",
      "Epoch[16] 8000/8000 : Cost = 0.3245 ; Acc = 83.35% ; Time Taken = 0s\n",
      "\n",
      "Epoch[17] 8000/8000 : Cost = 0.3236 ; Acc = 82.86% ; Time Taken = 0s\n",
      "\n",
      "Epoch[18] 8000/8000 : Cost = 0.3233 ; Acc = 82.99% ; Time Taken = 0s\n",
      "\n",
      "Epoch[19] 8000/8000 : Cost = 0.3225 ; Acc = 84.11% ; Time Taken = 0s\n",
      "\n",
      "Epoch[20] 8000/8000 : Cost = 0.3220 ; Acc = 82.73% ; Time Taken = 0s\n",
      "\n",
      "Epoch[21] 8000/8000 : Cost = 0.3210 ; Acc = 83.53% ; Time Taken = 0s\n",
      "\n",
      "Epoch[22] 8000/8000 : Cost = 0.3205 ; Acc = 83.14% ; Time Taken = 0s\n",
      "\n",
      "Epoch[23] 8000/8000 : Cost = 0.3201 ; Acc = 83.50% ; Time Taken = 0s\n",
      "\n",
      "Epoch[24] 8000/8000 : Cost = 0.3188 ; Acc = 83.34% ; Time Taken = 0s\n",
      "\n",
      "Epoch[25] 8000/8000 : Cost = 0.3196 ; Acc = 83.08% ; Time Taken = 0s\n",
      "\n",
      "Epoch[26] 8000/8000 : Cost = 0.3185 ; Acc = 83.44% ; Time Taken = 0s\n",
      "\n",
      "Epoch[27] 8000/8000 : Cost = 0.3179 ; Acc = 83.03% ; Time Taken = 0s\n",
      "\n",
      "Epoch[28] 8000/8000 : Cost = 0.3181 ; Acc = 83.46% ; Time Taken = 0s\n",
      "\n",
      "Epoch[29] 8000/8000 : Cost = 0.3161 ; Acc = 83.43% ; Time Taken = 0s\n",
      "\n",
      "Epoch[30] 8000/8000 : Cost = 0.3167 ; Acc = 83.06% ; Time Taken = 0s\n",
      "\n",
      "Epoch[31] 8000/8000 : Cost = 0.3161 ; Acc = 83.93% ; Time Taken = 0s\n",
      "\n",
      "Epoch[32] 8000/8000 : Cost = 0.3155 ; Acc = 83.35% ; Time Taken = 0s\n",
      "\n",
      "Epoch[33] 8000/8000 : Cost = 0.3162 ; Acc = 83.74% ; Time Taken = 0s\n",
      "\n",
      "Epoch[34] 8000/8000 : Cost = 0.3141 ; Acc = 83.79% ; Time Taken = 0s\n",
      "\n",
      "Epoch[35] 8000/8000 : Cost = 0.3161 ; Acc = 83.01% ; Time Taken = 0s\n",
      "\n",
      "Epoch[36] 8000/8000 : Cost = 0.3151 ; Acc = 83.81% ; Time Taken = 0s\n",
      "\n",
      "Epoch[37] 8000/8000 : Cost = 0.3138 ; Acc = 83.78% ; Time Taken = 0s\n",
      "\n",
      "Epoch[38] 8000/8000 : Cost = 0.3139 ; Acc = 83.67% ; Time Taken = 0s\n",
      "\n",
      "Epoch[39] 8000/8000 : Cost = 0.3128 ; Acc = 84.46% ; Time Taken = 0s\n",
      "\n",
      "Epoch[40] 8000/8000 : Cost = 0.3132 ; Acc = 83.46% ; Time Taken = 0s\n",
      "\n",
      "Epoch[41] 8000/8000 : Cost = 0.3125 ; Acc = 84.08% ; Time Taken = 0s\n",
      "\n",
      "Epoch[42] 8000/8000 : Cost = 0.3119 ; Acc = 83.69% ; Time Taken = 0s\n",
      "\n",
      "Epoch[43] 8000/8000 : Cost = 0.3122 ; Acc = 84.00% ; Time Taken = 0s\n",
      "\n",
      "Epoch[44] 8000/8000 : Cost = 0.3107 ; Acc = 84.21% ; Time Taken = 0s\n",
      "\n",
      "Epoch[45] 8000/8000 : Cost = 0.3114 ; Acc = 83.76% ; Time Taken = 0s\n",
      "\n",
      "Epoch[46] 8000/8000 : Cost = 0.3111 ; Acc = 84.26% ; Time Taken = 0s\n",
      "\n",
      "Epoch[47] 8000/8000 : Cost = 0.3108 ; Acc = 83.28% ; Time Taken = 0s\n",
      "\n",
      "Epoch[48] 8000/8000 : Cost = 0.3106 ; Acc = 84.10% ; Time Taken = 0s\n",
      "\n",
      "Epoch[49] 8000/8000 : Cost = 0.3093 ; Acc = 84.58% ; Time Taken = 0s\n",
      "\n",
      "Epoch[50] 8000/8000 : Cost = 0.3107 ; Acc = 82.97% ; Time Taken = 0s\n",
      "\n",
      "Epoch[51] 8000/8000 : Cost = 0.3092 ; Acc = 84.11% ; Time Taken = 0s\n",
      "\n",
      "Epoch[52] 8000/8000 : Cost = 0.3089 ; Acc = 83.70% ; Time Taken = 0s\n",
      "\n",
      "Epoch[53] 8000/8000 : Cost = 0.3086 ; Acc = 84.40% ; Time Taken = 0s\n",
      "\n",
      "Epoch[54] 8000/8000 : Cost = 0.3075 ; Acc = 84.21% ; Time Taken = 0s\n",
      "\n",
      "Epoch[55] 8000/8000 : Cost = 0.3085 ; Acc = 83.56% ; Time Taken = 0s\n",
      "\n",
      "Epoch[56] 8000/8000 : Cost = 0.3085 ; Acc = 84.76% ; Time Taken = 0s\n",
      "\n",
      "Epoch[57] 8000/8000 : Cost = 0.3079 ; Acc = 83.67% ; Time Taken = 0s\n",
      "\n",
      "Epoch[58] 8000/8000 : Cost = 0.3080 ; Acc = 84.66% ; Time Taken = 0s\n",
      "\n",
      "Epoch[59] 8000/8000 : Cost = 0.3068 ; Acc = 84.65% ; Time Taken = 0s\n",
      "\n",
      "Epoch[60] 8000/8000 : Cost = 0.3076 ; Acc = 83.40% ; Time Taken = 0s\n",
      "\n",
      "Epoch[61] 8000/8000 : Cost = 0.3071 ; Acc = 84.79% ; Time Taken = 0s\n",
      "\n",
      "Epoch[62] 8000/8000 : Cost = 0.3066 ; Acc = 83.53% ; Time Taken = 0s\n",
      "\n",
      "Epoch[63] 8000/8000 : Cost = 0.3066 ; Acc = 84.59% ; Time Taken = 0s\n",
      "\n",
      "Epoch[64] 8000/8000 : Cost = 0.3051 ; Acc = 84.50% ; Time Taken = 0s\n",
      "\n",
      "Epoch[65] 8000/8000 : Cost = 0.3070 ; Acc = 83.15% ; Time Taken = 0s\n",
      "\n",
      "Epoch[66] 8000/8000 : Cost = 0.3063 ; Acc = 84.94% ; Time Taken = 0s\n",
      "\n",
      "Epoch[67] 8000/8000 : Cost = 0.3057 ; Acc = 83.99% ; Time Taken = 0s\n",
      "\n",
      "Epoch[68] 8000/8000 : Cost = 0.3054 ; Acc = 84.79% ; Time Taken = 0s\n",
      "\n",
      "Epoch[69] 8000/8000 : Cost = 0.3051 ; Acc = 84.76% ; Time Taken = 0s\n",
      "\n",
      "Epoch[70] 8000/8000 : Cost = 0.3050 ; Acc = 84.00% ; Time Taken = 0s\n",
      "\n",
      "Epoch[71] 8000/8000 : Cost = 0.3049 ; Acc = 84.91% ; Time Taken = 0s\n",
      "\n",
      "Epoch[72] 8000/8000 : Cost = 0.3047 ; Acc = 83.51% ; Time Taken = 0s\n",
      "\n",
      "Epoch[73] 8000/8000 : Cost = 0.3040 ; Acc = 84.65% ; Time Taken = 0s\n",
      "\n",
      "Epoch[74] 8000/8000 : Cost = 0.3033 ; Acc = 84.82% ; Time Taken = 0s\n",
      "\n",
      "Epoch[75] 8000/8000 : Cost = 0.3045 ; Acc = 83.83% ; Time Taken = 0s\n",
      "\n",
      "Epoch[76] 8000/8000 : Cost = 0.3045 ; Acc = 85.05% ; Time Taken = 0s\n",
      "\n",
      "Epoch[77] 8000/8000 : Cost = 0.3033 ; Acc = 84.12% ; Time Taken = 0s\n",
      "\n",
      "Epoch[78] 8000/8000 : Cost = 0.3040 ; Acc = 85.00% ; Time Taken = 0s\n",
      "\n",
      "Epoch[79] 8000/8000 : Cost = 0.3025 ; Acc = 84.50% ; Time Taken = 0s\n",
      "\n",
      "Epoch[80] 8000/8000 : Cost = 0.3036 ; Acc = 83.60% ; Time Taken = 0s\n",
      "\n",
      "Epoch[81] 8000/8000 : Cost = 0.3032 ; Acc = 85.02% ; Time Taken = 0s\n",
      "\n",
      "Epoch[82] 8000/8000 : Cost = 0.3026 ; Acc = 84.15% ; Time Taken = 0s\n",
      "\n",
      "Epoch[83] 8000/8000 : Cost = 0.3032 ; Acc = 85.11% ; Time Taken = 0s\n",
      "\n",
      "Epoch[84] 8000/8000 : Cost = 0.3020 ; Acc = 85.00% ; Time Taken = 0s\n",
      "\n",
      "Epoch[85] 8000/8000 : Cost = 0.3029 ; Acc = 83.83% ; Time Taken = 0s\n",
      "\n",
      "Epoch[86] 8000/8000 : Cost = 0.3031 ; Acc = 85.25% ; Time Taken = 0s\n",
      "\n",
      "Epoch[87] 8000/8000 : Cost = 0.3017 ; Acc = 84.39% ; Time Taken = 0s\n",
      "\n",
      "Epoch[88] 8000/8000 : Cost = 0.3022 ; Acc = 85.10% ; Time Taken = 0s\n",
      "\n",
      "Epoch[89] 8000/8000 : Cost = 0.3015 ; Acc = 84.99% ; Time Taken = 0s\n",
      "\n",
      "Epoch[90] 8000/8000 : Cost = 0.3025 ; Acc = 83.73% ; Time Taken = 0s\n",
      "\n",
      "Epoch[91] 8000/8000 : Cost = 0.3021 ; Acc = 85.10% ; Time Taken = 0s\n",
      "\n",
      "Epoch[92] 8000/8000 : Cost = 0.3013 ; Acc = 83.71% ; Time Taken = 0s\n",
      "\n",
      "Epoch[93] 8000/8000 : Cost = 0.3019 ; Acc = 85.10% ; Time Taken = 0s\n",
      "\n",
      "Epoch[94] 8000/8000 : Cost = 0.3006 ; Acc = 84.96% ; Time Taken = 0s\n",
      "\n",
      "Epoch[95] 8000/8000 : Cost = 0.3015 ; Acc = 83.79% ; Time Taken = 0s\n",
      "\n",
      "Epoch[96] 8000/8000 : Cost = 0.3015 ; Acc = 85.35% ; Time Taken = 0s\n",
      "\n",
      "Epoch[97] 8000/8000 : Cost = 0.3005 ; Acc = 84.05% ; Time Taken = 0s\n",
      "\n",
      "Epoch[98] 8000/8000 : Cost = 0.3015 ; Acc = 85.28% ; Time Taken = 0s\n",
      "\n",
      "Epoch[99] 8000/8000 : Cost = 0.3002 ; Acc = 85.15% ; Time Taken = 0s\n",
      "\n",
      "Epoch[100] 8000/8000 : Cost = 0.3008 ; Acc = 83.89% ; Time Taken = 0s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initializing our neural network\n",
    "neural_net = NeuralNet([32,32,1], X_train, y_train, ac_funcs = ['relu','relu','sigmoid'])\n",
    "# Staring the training of our network.\n",
    "neural_net.startTraining(batch_size=500, epochs=100, alpha=0.01, decay_rate=0.0001, beta1=0.9, beta2=0.999, _lambda=0.7, keep_prob=[0.9,0.9], threshold=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We are still getting a good accuracy of over 80% with threshold as low as 25%! (which is awesome!) Now, let's see how the cost of the network decreased during training and then move forward to evaluate our model on a test set.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Plotting our results! ###\n",
    "<p> As seen, we trained a pretty good network and it's the time to see if our implementation was correct. If the cost consistently decreases with a little bit of noise than our implemetation is perfect! Otherwise, we need to go back to see which mistakes have we made.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting our cost vs epochs relationship\n",
    "summary = neural_net.summary\n",
    "plt.plot(range(len(summary[0])), summary[0])\n",
    "plt.title('Cost vs Steps')\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the cost does consistently decrease with a little bit of noise at the end. This means our implemetation is correct and we have sucessfully trained a network to perform churn predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluating our Model ###\n",
    "<p> Having trained our first neural network, let's see how good it does on the test set. We have had a lot of disscussion on the metric to use to evaluate our model and finally choose 'recall' as our optimizing metric. After training a lot of networks, below is the result I obtained as the most optimized one.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.84      0.87       769\n",
      "          1       0.57      0.72      0.63       231\n",
      "\n",
      "avg / total       0.83      0.81      0.82      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = 1*(neural_net.predict(X_test) > 0.25)\n",
    "print(classification_report(y_test.T, y_pred.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have obtained a pretty high recall with high enough F1 score and precision. Now, let's proceed ahead to evaluate our dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.84      0.88       805\n",
      "          1       0.51      0.71      0.59       195\n",
      "\n",
      "avg / total       0.84      0.81      0.82      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = 1*(neural_net.predict(X_CV) > 0.25)\n",
    "print(classification_report(y_CV.T, y_pred.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stolen form Stack Overflow\n",
    "fpr, tpr, threshold = roc_curve(y_CV.T, y_pred.T)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, 'b', label = f'NN AUC = {roc_auc:.2f}')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the recall for both test and dev sets are pretty high and has barely hurt the performance of the network. We also get a good AUC curve which shows that our model is pretty good. You can further improve the network to increase the recall of the model and maintain all other satisficing metrics. There are a lot of ways to do it, out of which, I choose the way I really liked. There are more powerful classifiers for churn modelling like naive bayes and logistic regression. Try them out and most importantly... Peace Out!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
