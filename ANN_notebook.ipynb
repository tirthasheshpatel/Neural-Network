{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore');\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/Hilak/Desktop/INTERESTS/Machine Learning A-Z Template Folder/Part 3 - Classification/Section 14 - Logistic Regression\");\n",
    "training_set = pd.read_csv(\"Social_Network_Ads.csv\");\n",
    "X = training_set.iloc[:, 1:4].values\n",
    "y = training_set.iloc[:, 4].values\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "le_x = LabelEncoder()\n",
    "X[:,0] = le_x.fit_transform(X[:,0])\n",
    "ohe = OneHotEncoder(categorical_features = [0])\n",
    "X = ohe.fit_transform(X).toarray()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "X[:,2:4] = ss.fit_transform(X[:, 2:4])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate Dataset for test purposes. Not used in the example shown\n",
    "os.chdir(\"C:\\\\Users\\\\Hilak\\\\Desktop\\\\INTERESTS\\\\Machine Learning A-Z Template Folder\\\\Part 8 - Deep Learning\\\\Section 39 - Artificial Neural Networks (ANN)\");\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "\n",
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "X_test, X_CV, y_test, y_CV = train_test_split(X, y, test_size = 0.5)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "X_CV = X_CV.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z) : \n",
    "    return 1./(1 + np.exp(-z))\n",
    "def sigmoid_prime(z) :\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "def ReLU(z) : \n",
    "    return (z*(z > 0))\n",
    "def ReLU_prime(z) :\n",
    "    return 1*(z>=0)\n",
    "def lReLU(z) : \n",
    "    return np.maximum(z/100,z)\n",
    "def lReLU_prime(z) :\n",
    "    z = 1*(z>=0)\n",
    "    z[z==0] = 1/100\n",
    "    return z\n",
    "def tanh(z) : \n",
    "    return np.tanh(z)\n",
    "def tanh_prime(z) : \n",
    "    return (1-tanh(z)**2)\n",
    "PHI = {'sigmoid':sigmoid, 'relu':ReLU, 'lrelu':lReLU, 'tanh':tanh}\n",
    "PHI_PRIME = {'sigmoid':sigmoid_prime, 'relu':ReLU_prime, 'lrelu':lReLU_prime, 'tanh':tanh_prime}\n",
    "class NeuralNet : \n",
    "    def __init__(self, layers, X, y, ac_funcs, init_method='gaussian', loss_func='b_ce', W=np.array([]), B=np.array([])) : \n",
    "        self.layers = layers\n",
    "        self.W = None\n",
    "        self.B = None\n",
    "        self.m = X.shape[1]\n",
    "        self.n = [X.shape[0], *layers]\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.cost = []\n",
    "        self.acc = 0\n",
    "        self.ac_funcs = ac_funcs\n",
    "        self.loss = loss_func\n",
    "        if len(W) and len(B) :\n",
    "            self.W = W\n",
    "            self.B = B\n",
    "        else : \n",
    "            if init_method=='gaussian': \n",
    "                self.W = [np.random.randn(self.n[nl], self.n[nl-1]) for nl in range(1,len(self.n))]\n",
    "                self.B = [np.random.randn(nl,1) for nl in self.layers]\n",
    "            elif init_method == 'random':\n",
    "                self.W = [np.random.rand(self.n[nl], self.n[nl-1]) for nl in range(1,len(self.n))]\n",
    "                self.B = [np.random.rand(nl,1) for nl in self.layers]\n",
    "            elif init_method == 'zeros':\n",
    "                self.W = [np.zeros((self.n[nl], self.n[nl-1]), 'float32') for nl in range(1,len(self.n))]\n",
    "                self.B = [np.zeros((nl,1), 'float32') for nl in self.layers]\n",
    "    \n",
    "    def startTraining(self, epochs, alpha, interval=100):\n",
    "        start = time.time()\n",
    "        for i in range(epochs+1) : \n",
    "            z,a = self._feedForward()\n",
    "            delta = self._cost_derivative(a[-1])\n",
    "            for l in range(1,len(z)) : \n",
    "                delta_w = np.dot(delta, a[-l-1].T)\n",
    "                delta_b = np.dot(delta, np.ones((self.m,1), 'int32'))\n",
    "                self.W[-l] = self.W[-l] - (alpha/self.m)*delta_w\n",
    "                self.B[-l] = self.B[-l] - (alpha/self.m)*delta_b\n",
    "                delta = np.dot(self.W[-l].T, delta)*PHI_PRIME[self.ac_funcs[-l]](z[-l-1])\n",
    "            if not i%interval :\n",
    "                aa = self.predict(self.X)\n",
    "                aa = aa > 0.5\n",
    "                self.acc = sum(sum(aa == self.y)) / self.m\n",
    "                cost_val = self._cost_func(a[-1])\n",
    "                self.cost.append(cost_val)\n",
    "            sys.stdout.write(f'\\rEpoch[{i}] : Cost = {cost_val:.2f} ; Acc = {(self.acc*100):.2f}% ; Time Taken = {(time.time()-start):.2f}s')\n",
    "        print('\\n')\n",
    "        return None\n",
    "    \n",
    "    def predict(self, X_test) : \n",
    "        a = PHI[self.ac_funcs[0]](np.dot(self.W[0], X_test) + self.B[0])\n",
    "        for l in range(1,len(self.layers)):\n",
    "            a = PHI[self.ac_funcs[l]](np.dot(self.W[l], a) + self.B[l])\n",
    "        return a\n",
    "            \n",
    "    \n",
    "    def _feedForward(self):\n",
    "        z = [];a = []\n",
    "        z.append(np.dot(self.W[0], self.X) + self.B[0])\n",
    "        a.append(PHI[self.ac_funcs[0]](z[0]))\n",
    "        for l in range(1,len(self.layers)):\n",
    "            z.append(np.dot(self.W[l], a[l-1]) + self.B[l])\n",
    "            a.append(PHI[self.ac_funcs[l]](z[l]))\n",
    "        return z,a\n",
    "    \n",
    "    def _cost_func(self, a):\n",
    "        return ( (-1/self.m)*np.sum(np.nan_to_num(self.y*np.log(a) + (1-self.y)*np.log(1-a))) )\n",
    "\n",
    "    def _cost_derivative(self, a) : \n",
    "        return a-self.y\n",
    "   \n",
    "    @property\n",
    "    def summary(self) :\n",
    "        return self.cost, self.acc, self.W,self.B\n",
    "    def __repr__(self) : \n",
    "        return f'<UNDER CONST>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[10000] : Cost = 0.16 ; Acc = 94.29% ; Time Taken = 32.13s\n",
      "\n",
      "Test set Accuracy ( r-t-s ) : 89.16666666666667%\n"
     ]
    }
   ],
   "source": [
    "neural_net_sigmoid = NeuralNet([32,16,1], X_train, y_train, ac_funcs = ['tanh','relu','sigmoid'])\n",
    "neural_net_sigmoid.startTraining(10000, 0.02, 100)\n",
    "preds = neural_net_sigmoid.predict(X_test)\n",
    "preds = preds > 0.5\n",
    "acc = (sum(sum(preds == y_test)) / y_test.size)*100\n",
    "print(f'Test set Accuracy ( r-t-s ) : {acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sigmoid_summary = neural_net_sigmoid.summary\n",
    "plt.plot(range(len(sigmoid_summary[0])), sigmoid_summary[0], label='Sigmoid Cost')\n",
    "plt.title('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "280/280 [==============================] - 3s 10ms/step - loss: 0.5968 - acc: 0.7321\n",
      "Epoch 2/50\n",
      "280/280 [==============================] - 2s 6ms/step - loss: 0.3591 - acc: 0.8250\n",
      "Epoch 3/50\n",
      "280/280 [==============================] - 2s 5ms/step - loss: 0.2977 - acc: 0.8750\n",
      "Epoch 4/50\n",
      "280/280 [==============================] - 2s 5ms/step - loss: 0.2777 - acc: 0.8929\n",
      "Epoch 5/50\n",
      "280/280 [==============================] - 1s 5ms/step - loss: 0.2665 - acc: 0.8964\n",
      "Epoch 6/50\n",
      "280/280 [==============================] - 2s 5ms/step - loss: 0.2586 - acc: 0.9107\n",
      "Epoch 7/50\n",
      "280/280 [==============================] - 2s 6ms/step - loss: 0.2469 - acc: 0.9071\n",
      "Epoch 8/50\n",
      "280/280 [==============================] - 2s 6ms/step - loss: 0.2394 - acc: 0.9071\n",
      "Epoch 9/50\n",
      "280/280 [==============================] - 2s 5ms/step - loss: 0.2364 - acc: 0.9107\n",
      "Epoch 10/50\n",
      "280/280 [==============================] - 2s 6ms/step - loss: 0.2304 - acc: 0.9143\n",
      "Epoch 11/50\n",
      "280/280 [==============================] - 2s 6ms/step - loss: 0.2237 - acc: 0.9143\n",
      "Epoch 12/50\n",
      "280/280 [==============================] - 1s 5ms/step - loss: 0.2223 - acc: 0.9143\n",
      "Epoch 13/50\n",
      "280/280 [==============================] - 1s 4ms/step - loss: 0.2201 - acc: 0.9179\n",
      "Epoch 14/50\n",
      "280/280 [==============================] - 1s 4ms/step - loss: 0.2178 - acc: 0.9143\n",
      "Epoch 15/50\n",
      "280/280 [==============================] - 1s 3ms/step - loss: 0.2175 - acc: 0.9179\n",
      "Epoch 16/50\n",
      "280/280 [==============================] - 1s 3ms/step - loss: 0.2157 - acc: 0.9143A: 0s - loss: 0.1577 \n",
      "Epoch 17/50\n",
      "280/280 [==============================] - 1s 3ms/step - loss: 0.2127 - acc: 0.9286\n",
      "Epoch 18/50\n",
      "280/280 [==============================] - 1s 3ms/step - loss: 0.2124 - acc: 0.9214\n",
      "Epoch 19/50\n",
      "280/280 [==============================] - 1s 3ms/step - loss: 0.2128 - acc: 0.9179\n",
      "Epoch 20/50\n",
      "280/280 [==============================] - 1s 3ms/step - loss: 0.2094 - acc: 0.9214\n",
      "Epoch 21/50\n",
      "280/280 [==============================] - 1s 3ms/step - loss: 0.2148 - acc: 0.9250\n",
      "Epoch 22/50\n",
      "280/280 [==============================] - 1s 3ms/step - loss: 0.2123 - acc: 0.9250\n",
      "Epoch 23/50\n",
      "280/280 [==============================] - 1s 3ms/step - loss: 0.2131 - acc: 0.9107\n",
      "Epoch 24/50\n",
      "280/280 [==============================] - 1s 3ms/step - loss: 0.2097 - acc: 0.9250\n",
      "Epoch 25/50\n",
      "280/280 [==============================] - 1s 3ms/step - loss: 0.2119 - acc: 0.9214\n",
      "Epoch 26/50\n",
      "280/280 [==============================] - 1s 3ms/step - loss: 0.2098 - acc: 0.9179\n",
      "Epoch 27/50\n",
      "280/280 [==============================] - 1s 3ms/step - loss: 0.2071 - acc: 0.9250\n",
      "Epoch 28/50\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.2025 - acc: 0.9179\n",
      "Epoch 29/50\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.2066 - acc: 0.9321\n",
      "Epoch 30/50\n",
      "280/280 [==============================] - 1s 3ms/step - loss: 0.2037 - acc: 0.9286\n",
      "Epoch 31/50\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.2057 - acc: 0.9214\n",
      "Epoch 32/50\n",
      "280/280 [==============================] - 1s 3ms/step - loss: 0.2030 - acc: 0.9321- ETA: 0s - loss: 0.1583 - acc: 0.947 - ETA: 0s - loss: 0.1891 - ac\n",
      "Epoch 33/50\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.2026 - acc: 0.9321\n",
      "Epoch 34/50\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.2002 - acc: 0.9250\n",
      "Epoch 35/50\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.1999 - acc: 0.9286\n",
      "Epoch 36/50\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.2022 - acc: 0.9250\n",
      "Epoch 37/50\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.1998 - acc: 0.9286\n",
      "Epoch 38/50\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.1992 - acc: 0.9250\n",
      "Epoch 39/50\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.1999 - acc: 0.9286\n",
      "Epoch 40/50\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.1969 - acc: 0.9321\n",
      "Epoch 41/50\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.1986 - acc: 0.9321\n",
      "Epoch 42/50\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.1952 - acc: 0.9321\n",
      "Epoch 43/50\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.1999 - acc: 0.9357\n",
      "Epoch 44/50\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.1958 - acc: 0.9214\n",
      "Epoch 45/50\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.1963 - acc: 0.9250\n",
      "Epoch 46/50\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.1980 - acc: 0.9250\n",
      "Epoch 47/50\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.1936 - acc: 0.9250\n",
      "Epoch 48/50\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.1922 - acc: 0.9321\n",
      "Epoch 49/50\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.1875 - acc: 0.9357A: 0s - loss: 0.1882 - acc: 0.935\n",
      "Epoch 50/50\n",
      "280/280 [==============================] - 1s 2ms/step - loss: 0.2017 - acc: 0.9250\n",
      "Test set Accuracy : 90.0%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "X_train, X_test = X_train.T, X_test.T\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(input_dim=4, units = 32, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
    "classifier.add(Dense(units = 16, kernel_initializer = \"uniform\", activation=\"relu\"))\n",
    "# classifier.add(Dense(units = 16, kernel_initializer = \"uniform\", activation=\"tanh\"))\n",
    "classifier.add(Dense(units = 1, kernel_initializer = \"uniform\", activation = \"sigmoid\"))\n",
    "classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "classifier.fit(X_train, y_train, batch_size = 1, epochs = 50)\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = 1*(y_pred > 0.5)\n",
    "test_acc = sum(sum(y_pred.T == y_test)) / y_test.size\n",
    "print(f\"Test set Accuracy : {test_acc*100}%\")\n",
    "X_train, X_test = X_train.T, X_test.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
